{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "import optuna\n",
    "\n",
    "# Kaggle setup\n",
    "output_dir = \"/kaggle/working/output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_parquet(\"/kaggle/input/climatetext/train.parquet\")\n",
    "df['label_int'] = df['label'].str.split(\"_\").str[0].astype('int')\n",
    "label_dict = df[['label_int', 'label']].drop_duplicates().set_index('label_int')['label'].to_dict()\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "class QuotesDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def encode_data(tokenizer, texts, labels, max_length):\n",
    "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "    return QuotesDataset(encodings, labels)\n",
    "\n",
    "texts = df[\"quote\"].to_list()\n",
    "labels = df[\"label_int\"].to_list()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    num_trainable_layers = trial.suggest_int('num_trainable_layers', 1, 6)\n",
    "    train_batch_size = trial.suggest_categorical('train_batch_size', [16, 32, 64])\n",
    "    grad_clip = trial.suggest_uniform('grad_clip', 0.5, 5.0)\n",
    "    step_size = trial.suggest_int('step_size', 5, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.7, 0.9)\n",
    "    max_length = trial.suggest_int('max_length', 128, 512)\n",
    "\n",
    "    # Dataset\n",
    "    train_dataset = encode_data(tokenizer, X_train, y_train, max_length)\n",
    "    val_dataset = encode_data(tokenizer, X_val, y_val, max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False)\n",
    "\n",
    "    # Model configuration\n",
    "    config = DistilBertConfig.from_pretrained(\n",
    "        'distilbert-base-uncased', \n",
    "        num_labels=len(label_dict),\n",
    "        dropout=dropout_rate,\n",
    "        attention_dropout=dropout_rate\n",
    "    )\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "    # Freeze layers except for the last `num_trainable_layers`\n",
    "    for name, param in model.distilbert.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for layer_idx in range(6 - num_trainable_layers, 6):\n",
    "        for name, param in model.distilbert.transformer.layer[layer_idx].named_parameters():\n",
    "            param.requires_grad = True\n",
    "    for name, param in model.classifier.named_parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Training loop (only 3 epochs for HPO efficiency)\n",
    "    epochs = 3\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        all_predictions, all_true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_true_labels.extend(batch['labels'].cpu().numpy())\n",
    "        val_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "        best_val_f1 = max(best_val_f1, val_f1)\n",
    "\n",
    "    return best_val_f1\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # Adjust the number of trials as needed\n",
    "\n",
    "# Save best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "with open(f\"{output_dir}/best_hyperparameters.txt\", \"w\") as f:\n",
    "    f.write(str(study.best_params))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

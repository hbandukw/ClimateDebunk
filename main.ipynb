{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10542548,"datasetId":6523213,"databundleVersionId":10874335},{"sourceType":"datasetVersion","sourceId":10471355,"datasetId":6483657,"databundleVersionId":10795098}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nlpaug\n!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:31:02.040286Z","iopub.execute_input":"2025-01-23T22:31:02.040617Z","iopub.status.idle":"2025-01-23T22:31:12.138749Z","shell.execute_reply.started":"2025-01-23T22:31:02.040581Z","shell.execute_reply":"2025-01-23T22:31:12.137928Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.12.14)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\nfrom sklearn.metrics import f1_score, confusion_matrix, balanced_accuracy_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.optim import AdamW, lr_scheduler\nimport nlpaug.augmenter.word as naw\nimport optuna\nimport shutil\nimport zipfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:31:16.743458Z","iopub.execute_input":"2025-01-23T22:31:16.743803Z","iopub.status.idle":"2025-01-23T22:32:10.883919Z","shell.execute_reply.started":"2025-01-23T22:31:16.743776Z","shell.execute_reply":"2025-01-23T22:32:10.883258Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Ensure output directory exists\noutput_dir = \"/kaggle/working/output\"\nos.makedirs(output_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:21:18.326270Z","iopub.execute_input":"2025-01-23T16:21:18.326858Z","iopub.status.idle":"2025-01-23T16:21:18.330629Z","shell.execute_reply.started":"2025-01-23T16:21:18.326835Z","shell.execute_reply":"2025-01-23T16:21:18.329784Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load and prepare data\n# original\ndf = pd.read_parquet(\"/kaggle/input/train-parquet\")\ndf['label_int'] = df['label'].str.split(\"_\").str[0].astype('int')\n\n# augmented \ntrain1 = pd.read_csv('/kaggle/input/balanced/train1.csv')\ntrain2 = pd.read_csv('/kaggle/input/balanced/train2.csv')\ntrain3 = pd.read_csv('/kaggle/input/balanced/train3.csv')\ntrain4 = pd.read_csv('/kaggle/input/balanced/train4.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:32:57.891546Z","iopub.execute_input":"2025-01-23T22:32:57.892265Z","iopub.status.idle":"2025-01-23T22:32:58.436681Z","shell.execute_reply.started":"2025-01-23T22:32:57.892236Z","shell.execute_reply":"2025-01-23T22:32:58.435793Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Split data\n# train123 is the augmentated data (the train set that; splitted with random state 42)\ntexts = df[\"quote\"].to_list()\nlabels = df[\"label_int\"].to_list()\n\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n\ndatasets = [train1, train2, train3, train4]\n\n# Extract quotes and labels using list comprehension\ntexts = [ds['quote'] for ds in datasets]\nlabels = [ds['numeric_label'] for ds in datasets]\n\n# Concatenate all texts and labels using pandas.concat\ntrain1234_texts = pd.concat(texts, ignore_index=True)\ntrain1234_labels = pd.concat(labels, ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:33:01.360348Z","iopub.execute_input":"2025-01-23T22:33:01.360686Z","iopub.status.idle":"2025-01-23T22:33:01.381433Z","shell.execute_reply.started":"2025-01-23T22:33:01.360659Z","shell.execute_reply":"2025-01-23T22:33:01.380631Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:33:04.194076Z","iopub.execute_input":"2025-01-23T22:33:04.194356Z","iopub.status.idle":"2025-01-23T22:33:04.281139Z","shell.execute_reply.started":"2025-01-23T22:33:04.194336Z","shell.execute_reply":"2025-01-23T22:33:04.280395Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Create dictionary for label names\nlabel_dict = df[['label_int', 'label']].drop_duplicates().set_index('label_int')['label'].to_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:33:06.637682Z","iopub.execute_input":"2025-01-23T22:33:06.637977Z","iopub.status.idle":"2025-01-23T22:33:06.658861Z","shell.execute_reply.started":"2025-01-23T22:33:06.637957Z","shell.execute_reply":"2025-01-23T22:33:06.657944Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\nMAX_LENGTH = 365\n\n# Dataset and DataLoader preparation\nclass QuotesDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ndef encode_data(tokenizer, texts, labels, max_length):\n    try:\n        if isinstance(texts, pd.Series):\n            texts = texts.tolist()\n        if isinstance(labels, pd.Series):\n            labels = labels.tolist()\n            \n        encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        return QuotesDataset(encodings, labels)\n\n    except Exception as e:\n        print(f\"Error during tokenization: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:34:25.350345Z","iopub.execute_input":"2025-01-23T22:34:25.350678Z","iopub.status.idle":"2025-01-23T22:34:25.453300Z","shell.execute_reply.started":"2025-01-23T22:34:25.350651Z","shell.execute_reply":"2025-01-23T22:34:25.452422Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"texts = df[\"quote\"].to_list()\nlabels = df[\"label_int\"].to_list()\n\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:34:27.750878Z","iopub.execute_input":"2025-01-23T22:34:27.751190Z","iopub.status.idle":"2025-01-23T22:34:27.762413Z","shell.execute_reply.started":"2025-01-23T22:34:27.751164Z","shell.execute_reply":"2025-01-23T22:34:27.761469Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_dataset = encode_data(tokenizer, X_train, y_train, MAX_LENGTH)\ntrain1234_dataset = encode_data(tokenizer, train1234_texts, train1234_labels, MAX_LENGTH)\nval_dataset = encode_data(tokenizer, X_test, y_test, MAX_LENGTH)\n\n# train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n# train1234_loader = DataLoader(train1234_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:34:29.121353Z","iopub.execute_input":"2025-01-23T22:34:29.121713Z","iopub.status.idle":"2025-01-23T22:34:50.409842Z","shell.execute_reply.started":"2025-01-23T22:34:29.121686Z","shell.execute_reply":"2025-01-23T22:34:50.408852Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"- Above is the basic data setup. Now train + hyperparam tune the model\n- Start with defining some functions to be used in objective","metadata":{}},{"cell_type":"code","source":"# Changes the layers freezed and drop out rate \n# def modify_model(model, num_trainable_layers, dropout_rate):\n#     # Freeze layers: Only the last 'num_trainable_layers' are trainable\n#     for name, param in model.named_parameters():\n#         if 'transformer.layer' in name:\n#             layer_num = int(name.split('.')[3])\n#             if layer_num < 6 - num_trainable_layers:\n#                 param.requires_grad = False\n\n#     # Adjust dropout rates in transformer layers\n#     for layer in model.distilbert.transformer.layer:\n#         layer.attention.dropout.p = dropout_rate\n#         layer.sa_layer_norm.dropout.p = dropout_rate\n#         layer.ffn.dropout.p = dropout_rate\n#         layer.output_layer_norm.dropout.p = dropout_rate\n\n#     return model\n\ndef modify_model(model, num_trainable_layers, dropout_rate):\n    # Freeze layers: only the last 'num_trainable_layers' are trainable\n    total_layers = len(model.distilbert.transformer.layer)\n    for layer_index, layer in enumerate(model.distilbert.transformer.layer):\n        if layer_index < total_layers - num_trainable_layers:\n            for param in layer.parameters():\n                param.requires_grad = False\n\n    # Adjust dropout rates in applicable transformer layers\n    for layer in model.distilbert.transformer.layer:\n        layer.attention.dropout.p = dropout_rate\n        layer.ffn.dropout.p = dropout_rate\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:39:08.864926Z","iopub.execute_input":"2025-01-23T22:39:08.865257Z","iopub.status.idle":"2025-01-23T22:39:08.870083Z","shell.execute_reply.started":"2025-01-23T22:39:08.865234Z","shell.execute_reply":"2025-01-23T22:39:08.869224Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Model training \ndef train_one_epoch(model, train_loader, optimizer, device):\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n    average_loss = train_loss / len(train_loader)\n    accuracy = correct_train / total_train\n    return average_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:39:11.330203Z","iopub.execute_input":"2025-01-23T22:39:11.330529Z","iopub.status.idle":"2025-01-23T22:39:11.335791Z","shell.execute_reply.started":"2025-01-23T22:39:11.330502Z","shell.execute_reply":"2025-01-23T22:39:11.334984Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Model validation \ndef validate_model(model, val_loader, device):\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    all_predictions = []\n    all_true_labels = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_true_labels.extend(batch['labels'].cpu().numpy())\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n    average_val_loss = val_loss / len(val_loader)\n    accuracy = correct_val / total_val\n    return average_val_loss, accuracy, all_predictions, all_true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T22:39:13.614096Z","iopub.execute_input":"2025-01-23T22:39:13.614382Z","iopub.status.idle":"2025-01-23T22:39:13.620125Z","shell.execute_reply.started":"2025-01-23T22:39:13.614361Z","shell.execute_reply":"2025-01-23T22:39:13.619305Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Hyperparam tune\ndef objective(trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n    num_trainable_layers = trial.suggest_int('num_trainable_layers', 1, 6)\n    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n    step_size = trial.suggest_int('step_size', 1, 10)\n    gamma = trial.suggest_float('gamma', 0.1, 0.9)\n    epochs = trial.suggest_int('epochs', 2, 5)  # Allowing optimization of number of epochs\n\n    # Model setup and modification\n    model_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n    model = DistilBertForSequenceClassification(model_config)\n    model = modify_model(model, num_trainable_layers, dropout_rate)\n    model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n    # Training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n    for epoch in range(epochs):\n        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, device)\n        val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n        scheduler.step()\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_accuracies.append(train_accuracy)\n        val_accuracies.append(val_accuracy)\n\n    results = []\n    for epoch in range(epochs):\n        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, device)\n        val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n        scheduler.step()\n\n        # Collect metrics\n        results.append({\n            'epoch': epoch + 1,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'train_accuracy': train_accuracy,\n            'val_accuracy': val_accuracy,\n            'trial_number': trial.number\n        })\n\n    # Store results in the global dictionary\n    results[trial.number] = pd.DataFrame(results)\n\n    # Optimize on the last validation loss\n    return results[-1]['val_accuracy']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T23:10:11.948205Z","iopub.execute_input":"2025-01-23T23:10:11.948517Z","iopub.status.idle":"2025-01-23T23:10:11.956817Z","shell.execute_reply.started":"2025-01-23T23:10:11.948496Z","shell.execute_reply":"2025-01-23T23:10:11.956049Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n\nprint(\"Best trial:\")\nprint(study.best_trial.params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T23:10:14.469094Z","iopub.execute_input":"2025-01-23T23:10:14.469380Z"}},"outputs":[{"name":"stderr","text":"[I 2025-01-23 23:10:14,470] A new study created in memory with name: no-name-58b230dc-7ebb-4df2-b0c3-736507b4db7b\n<ipython-input-24-a7c662dbfb68>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n<ipython-input-24-a7c662dbfb68>:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n<ipython-input-15-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n[I 2025-01-23 23:34:42,154] Trial 0 finished with value: 0.26579163248564397 and parameters: {'learning_rate': 1.0124547597577497e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.38432401077543454, 'batch_size': 64, 'step_size': 1, 'gamma': 0.26577707674707174, 'epochs': 5}. Best is trial 0 with value: 0.26579163248564397.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"for trial_num, results_df in trial_results.items():\n    print(f\"Results for Trial {trial_num}:\")\n    display(results_df)  # Using `display` for nicer formatting in Jupyter notebooks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the blow code once have best hyperparams ","metadata":{}},{"cell_type":"code","source":"# Training params\n# MAX_LENGTH = 365\nTRAIN_BATCH_SIZE = 16\nVAL_BATCH_SIZE = 64\nLEARNING_RATE = 1e-5\nSTEP_SIZE = 2\nGAMMA = 0.1\nEPOCHS = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:21:22.581826Z","iopub.execute_input":"2025-01-23T16:21:22.582055Z","iopub.status.idle":"2025-01-23T16:21:22.585765Z","shell.execute_reply.started":"2025-01-23T16:21:22.582036Z","shell.execute_reply":"2025-01-23T16:21:22.584895Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=8)\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:21:43.796173Z","iopub.execute_input":"2025-01-23T16:21:43.796375Z","iopub.status.idle":"2025-01-23T16:21:46.153996Z","shell.execute_reply.started":"2025-01-23T16:21:43.796356Z","shell.execute_reply":"2025-01-23T16:21:46.153353Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b548c46b2d548b19dff3f3ec1046f62"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Plotting function for accuracy\ndef plot_accuracies(training_accuracies, validation_accuracies):\n    plt.figure(figsize=(8, 6))\n    plt.plot(training_accuracies, label='Training Accuracy')\n    plt.plot(validation_accuracies, label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.savefig(f\"{output_dir}/accuracy_plot.png\")\n    plt.close()\n\ndef plot_confusion_matrix(cm, class_labels, epoch, output_dir):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n    plt.title(f'Confusion Matrix for Epoch {epoch + 1}')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.savefig(os.path.join(output_dir, f'confusion_matrix_epoch_{epoch + 1}.png'))\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:21:46.154761Z","iopub.execute_input":"2025-01-23T16:21:46.155056Z","iopub.status.idle":"2025-01-23T16:21:46.160402Z","shell.execute_reply.started":"2025-01-23T16:21:46.155028Z","shell.execute_reply":"2025-01-23T16:21:46.159574Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Training and validation loop\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\nmetrics_df = pd.DataFrame()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n\n    scheduler.step()\n    train_losses.append(train_loss / len(train_loader))\n    train_accuracies.append(correct_train / total_train)\n\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    all_predictions, all_true_labels = [], []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_true_labels.extend(batch['labels'].cpu().numpy())\n\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n\n    val_losses.append(val_loss / len(val_loader))\n    val_accuracies.append(correct_val / total_val)\n\n    # Compute confusion matrix\n    cm = confusion_matrix(all_true_labels, all_predictions)\n    class_labels = [label_dict.get(i, f'Class {i}') for i in range(len(np.unique(all_true_labels)))]\n    plot_confusion_matrix(cm, class_labels, epoch, output_dir)\n\n    # Calculate metrics\n    balanced_acc = balanced_accuracy_score(all_true_labels, all_predictions)\n    average_f1 = f1_score(all_true_labels, all_predictions, average='macro')\n    weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n    f1_scores_per_class = f1_score(all_true_labels, all_predictions, average=None)\n    precision_per_class = precision_score(all_true_labels, all_predictions, average=None, zero_division=0)\n    recall_per_class = recall_score(all_true_labels, all_predictions, average=None, zero_division=0)\n\n    # Append metrics to DataFrame with class labels\n    epoch_metrics = {\n        \"Epoch\": epoch + 1,\n        \"Train Loss\": train_losses[-1],\n        \"Validation Loss\": val_losses[-1],\n        \"Train Accuracy\": train_accuracies[-1],\n        \"Validation Accuracy\": val_accuracies[-1],\n        \"Balanced Accuracy\": balanced_acc,\n        \"Average F1\": average_f1,\n        \"Weighted F1\": weighted_f1\n    }\n    epoch_metrics.update({f\"{label_dict[i]} F1\": f1_scores_per_class[i] for i in range(len(f1_scores_per_class))})\n    epoch_metrics.update({f\"{label_dict[i]} Precision\": precision_per_class[i] for i in range(len(precision_per_class))})\n    epoch_metrics.update({f\"{label_dict[i]} Recall\": recall_per_class[i] for i in range(len(recall_per_class))})\n\n    metrics_df = pd.concat([metrics_df, pd.DataFrame([epoch_metrics])], ignore_index=True)\n\n# Save metrics to CSV and plot accuracies\nmetrics_df.to_csv(f\"{output_dir}/training_metrics.csv\", index=False)\nplot_accuracies(train_accuracies, val_accuracies)\n\n# Print overall model accuracy\noverall_accuracy = accuracy_score(y_test, all_predictions)\nprint(f\"Overall Model Accuracy: {overall_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:21:46.161213Z","iopub.execute_input":"2025-01-23T16:21:46.161536Z","iopub.status.idle":"2025-01-23T16:27:02.869184Z","shell.execute_reply.started":"2025-01-23T16:21:46.161503Z","shell.execute_reply":"2025-01-23T16:27:02.868333Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-c399c416995b>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n<ipython-input-8-c399c416995b>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"name":"stdout","text":"Overall Model Accuracy: 0.6390\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def zip_directory(folder_path, output_path):\n    \"\"\"Zip the contents of an entire directory and save the archive to the specified output path.\"\"\"\n    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(folder_path):\n            for file in files:\n                # Create a relative path for files to preserve the directory structure\n                zipf.write(os.path.join(root, file),\n                           os.path.relpath(os.path.join(root, file),\n                                           os.path.join(folder_path, '..')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:27:02.870078Z","iopub.execute_input":"2025-01-23T16:27:02.870371Z","iopub.status.idle":"2025-01-23T16:27:02.874997Z","shell.execute_reply.started":"2025-01-23T16:27:02.870347Z","shell.execute_reply":"2025-01-23T16:27:02.874123Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Directory to be zipped\ninput_dir = '/kaggle/working/output'\n\n# Output path for the zip file\nzip_file_path = '/kaggle/working/output.zip'\n\n# Creating the ZIP file\nzip_directory(input_dir, zip_file_path)\n\nprint(f\"Created zip file at: {zip_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:27:02.875852Z","iopub.execute_input":"2025-01-23T16:27:02.876196Z","iopub.status.idle":"2025-01-23T16:27:02.902889Z","shell.execute_reply.started":"2025-01-23T16:27:02.876166Z","shell.execute_reply":"2025-01-23T16:27:02.902226Z"}},"outputs":[{"name":"stdout","text":"Created zip file at: /kaggle/working/output.zip\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n\nprint(\"Best trial:\")\nprint(study.best_trial.params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:27:02.964126Z","iopub.execute_input":"2025-01-23T16:27:02.964374Z","iopub.status.idle":"2025-01-23T17:59:47.991119Z","shell.execute_reply.started":"2025-01-23T16:27:02.964344Z","shell.execute_reply":"2025-01-23T17:59:47.990362Z"}},"outputs":[{"name":"stderr","text":"[I 2025-01-23 16:27:02,977] A new study created in memory with name: no-name-33feeede-680b-4b4b-b5d3-0c2030f28365\n<ipython-input-19-b5ae6a325a6b>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n<ipython-input-19-b5ae6a325a6b>:46: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n<ipython-input-8-c399c416995b>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n[I 2025-01-23 16:32:05,315] Trial 0 finished with value: 1.9802471280097962 and parameters: {'learning_rate': 0.0006284216546763071, 'num_trainable_layers': 5, 'dropout_rate': 0.31673517108116045, 'batch_size': 64}. Best is trial 0 with value: 1.9802471280097962.\n[I 2025-01-23 16:37:25,909] Trial 1 finished with value: 1.6973186437185708 and parameters: {'learning_rate': 0.00011913816043207192, 'num_trainable_layers': 6, 'dropout_rate': 0.30590212974883535, 'batch_size': 16}. Best is trial 1 with value: 1.6973186437185708.\n[I 2025-01-23 16:42:40,135] Trial 2 finished with value: 1.6355553328990937 and parameters: {'learning_rate': 2.709873294536626e-05, 'num_trainable_layers': 6, 'dropout_rate': 0.3622829394064401, 'batch_size': 64}. Best is trial 2 with value: 1.6355553328990937.\n[I 2025-01-23 16:47:47,869] Trial 3 finished with value: 1.3759440802908562 and parameters: {'learning_rate': 6.189057946408627e-05, 'num_trainable_layers': 5, 'dropout_rate': 0.2093575856052895, 'batch_size': 16}. Best is trial 3 with value: 1.3759440802908562.\n[I 2025-01-23 16:52:28,960] Trial 4 finished with value: 1.3356755043005015 and parameters: {'learning_rate': 0.0001413214126888871, 'num_trainable_layers': 3, 'dropout_rate': 0.19476802106097435, 'batch_size': 16}. Best is trial 4 with value: 1.3356755043005015.\n[I 2025-01-23 16:57:08,179] Trial 5 finished with value: 1.5318366646766663 and parameters: {'learning_rate': 0.000437516573447868, 'num_trainable_layers': 3, 'dropout_rate': 0.1328340933371308, 'batch_size': 64}. Best is trial 4 with value: 1.3356755043005015.\n[I 2025-01-23 17:01:46,860] Trial 6 finished with value: 1.961612832546234 and parameters: {'learning_rate': 0.0006900288543940562, 'num_trainable_layers': 3, 'dropout_rate': 0.12018375012297407, 'batch_size': 64}. Best is trial 4 with value: 1.3356755043005015.\n[I 2025-01-23 17:06:49,762] Trial 7 finished with value: 1.506905698776245 and parameters: {'learning_rate': 4.0321325493183854e-05, 'num_trainable_layers': 5, 'dropout_rate': 0.20408536706317415, 'batch_size': 64}. Best is trial 4 with value: 1.3356755043005015.\n[I 2025-01-23 17:11:52,795] Trial 8 finished with value: 1.752838921546936 and parameters: {'learning_rate': 3.272323760199215e-05, 'num_trainable_layers': 5, 'dropout_rate': 0.45387290085807375, 'batch_size': 64}. Best is trial 4 with value: 1.3356755043005015.\n[I 2025-01-23 17:16:20,829] Trial 9 finished with value: 1.738995772600174 and parameters: {'learning_rate': 4.689188886620761e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.387647975524656, 'batch_size': 64}. Best is trial 4 with value: 1.3356755043005015.\n[I 2025-01-23 17:20:32,671] Trial 10 finished with value: 1.28637932966917 and parameters: {'learning_rate': 0.00020565551171102968, 'num_trainable_layers': 1, 'dropout_rate': 0.2227080957903141, 'batch_size': 32}. Best is trial 10 with value: 1.28637932966917.\n[I 2025-01-23 17:24:44,713] Trial 11 finished with value: 1.2933313036576295 and parameters: {'learning_rate': 0.00017511062104266762, 'num_trainable_layers': 1, 'dropout_rate': 0.22077158100003696, 'batch_size': 32}. Best is trial 10 with value: 1.28637932966917.\n[I 2025-01-23 17:28:56,534] Trial 12 finished with value: 1.4356287442720854 and parameters: {'learning_rate': 0.00027647093467717687, 'num_trainable_layers': 1, 'dropout_rate': 0.25298091981420906, 'batch_size': 32}. Best is trial 10 with value: 1.28637932966917.\n[I 2025-01-23 17:33:08,788] Trial 13 finished with value: 1.944822934957651 and parameters: {'learning_rate': 1.2666833134240303e-05, 'num_trainable_layers': 1, 'dropout_rate': 0.2590724719785798, 'batch_size': 32}. Best is trial 10 with value: 1.28637932966917.\n[I 2025-01-23 17:37:33,203] Trial 14 finished with value: 1.5312222425754254 and parameters: {'learning_rate': 0.00022314102020874594, 'num_trainable_layers': 2, 'dropout_rate': 0.14244625061018007, 'batch_size': 32}. Best is trial 10 with value: 1.28637932966917.\n[I 2025-01-23 17:41:45,108] Trial 15 finished with value: 1.3636578474289331 and parameters: {'learning_rate': 0.0002539404901245926, 'num_trainable_layers': 1, 'dropout_rate': 0.24524772840947046, 'batch_size': 32}. Best is trial 10 with value: 1.28637932966917.\n[I 2025-01-23 17:46:09,301] Trial 16 finished with value: 1.2686432523605151 and parameters: {'learning_rate': 9.047090823052945e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.16416468786842536, 'batch_size': 32}. Best is trial 16 with value: 1.2686432523605151.\n[I 2025-01-23 17:50:33,570] Trial 17 finished with value: 1.2789902228575487 and parameters: {'learning_rate': 7.970928572613957e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.16633540526351615, 'batch_size': 32}. Best is trial 16 with value: 1.2686432523605151.\n[I 2025-01-23 17:54:58,242] Trial 18 finished with value: 1.305173471952096 and parameters: {'learning_rate': 7.751836862694846e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.17287937867050468, 'batch_size': 32}. Best is trial 16 with value: 1.2686432523605151.\n[I 2025-01-23 17:59:47,986] Trial 19 finished with value: 1.601760949843969 and parameters: {'learning_rate': 2.2355596116895055e-05, 'num_trainable_layers': 4, 'dropout_rate': 0.1652140875756582, 'batch_size': 32}. Best is trial 16 with value: 1.2686432523605151.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n{'learning_rate': 9.047090823052945e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.16416468786842536, 'batch_size': 32}\n","output_type":"stream"}],"execution_count":21}]}
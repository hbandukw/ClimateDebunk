{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10542548,"datasetId":6523213,"databundleVersionId":10874335},{"sourceType":"datasetVersion","sourceId":10471355,"datasetId":6483657,"databundleVersionId":10795098}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nlpaug\n!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:20:26.625027Z","iopub.execute_input":"2025-01-23T16:20:26.625336Z","iopub.status.idle":"2025-01-23T16:20:34.642663Z","shell.execute_reply.started":"2025-01-23T16:20:26.625309Z","shell.execute_reply":"2025-01-23T16:20:34.641681Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.12.14)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom sklearn.metrics import f1_score, confusion_matrix, balanced_accuracy_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.optim import AdamW, lr_scheduler\nimport nlpaug.augmenter.word as naw\nimport optuna\nimport shutil\nimport zipfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:20:34.644039Z","iopub.execute_input":"2025-01-23T16:20:34.644255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure output directory exists\noutput_dir = \"/kaggle/working/output\"\nos.makedirs(output_dir, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and prepare data\n# original\ndf = pd.read_parquet(\"/kaggle/input/train-parquet\")\ndf['label_int'] = df['label'].str.split(\"_\").str[0].astype('int')\n\n# augmented \ntrain1 = pd.read_csv('/kaggle/input/balanced/train1.csv')\ntrain2 = pd.read_csv('/kaggle/input/balanced/train2.csv')\ntrain3 = pd.read_csv('/kaggle/input/balanced/train3.csv')\ntrain4 = pd.read_csv('/kaggle/input/balanced/train4.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data\n# train123 is the augmentated data (the train set that; splitted with random state 42)\ntexts = df[\"quote\"].to_list()\nlabels = df[\"label_int\"].to_list()\n\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n\ndatasets = [train1, train2, train3, train4]\n\n# Extract quotes and labels using list comprehension\ntexts = [ds['quote'] for ds in datasets]\nlabels = [ds['numeric_label'] for ds in datasets]\n\n# Concatenate all texts and labels using pandas.concat\ntrain1234_texts = pd.concat(texts, ignore_index=True)\ntrain1234_labels = pd.concat(labels, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create dictionary for label names\nlabel_dict = df[['label_int', 'label']].drop_duplicates().set_index('label_int')['label'].to_dict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n\n# Dataset and DataLoader preparation\nclass QuotesDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ndef encode_data(tokenizer, texts, labels, max_length):\n    try:\n        if isinstance(texts, pd.Series):\n            texts = texts.tolist()\n        if isinstance(labels, pd.Series):\n            labels = labels.tolist()\n            \n        encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        return QuotesDataset(encodings, labels)\n\n    except Exception as e:\n        print(f\"Error during tokenization: {e}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training params\nMAX_LENGTH = 365\nTRAIN_BATCH_SIZE = 16\nVAL_BATCH_SIZE = 64\nLEARNING_RATE = 1e-5\nSTEP_SIZE = 2\nGAMMA = 0.1\nEPOCHS = 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = df[\"quote\"].to_list()\nlabels = df[\"label_int\"].to_list()\n\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = encode_data(tokenizer, X_train, y_train, MAX_LENGTH)\ntrain1234_dataset = encode_data(tokenizer, train1234_texts, train1234_labels, MAX_LENGTH)\nval_dataset = encode_data(tokenizer, X_test, y_test, MAX_LENGTH)\n\ntrain_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\ntrain1234_loader = DataLoader(train1234_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=8)\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting function for accuracy\ndef plot_accuracies(training_accuracies, validation_accuracies):\n    plt.figure(figsize=(8, 6))\n    plt.plot(training_accuracies, label='Training Accuracy')\n    plt.plot(validation_accuracies, label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.savefig(f\"{output_dir}/accuracy_plot.png\")\n    plt.close()\n\ndef plot_confusion_matrix(cm, class_labels, epoch, output_dir):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n    plt.title(f'Confusion Matrix for Epoch {epoch + 1}')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.savefig(os.path.join(output_dir, f'confusion_matrix_epoch_{epoch + 1}.png'))\n    plt.close()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training and validation loop\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\nmetrics_df = pd.DataFrame()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n\n    scheduler.step()\n    train_losses.append(train_loss / len(train_loader))\n    train_accuracies.append(correct_train / total_train)\n\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    all_predictions, all_true_labels = [], []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_true_labels.extend(batch['labels'].cpu().numpy())\n\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n\n    val_losses.append(val_loss / len(val_loader))\n    val_accuracies.append(correct_val / total_val)\n\n    # Compute confusion matrix\n    cm = confusion_matrix(all_true_labels, all_predictions)\n    class_labels = [label_dict.get(i, f'Class {i}') for i in range(len(np.unique(all_true_labels)))]\n    plot_confusion_matrix(cm, class_labels, epoch, output_dir)\n\n    # Calculate metrics\n    balanced_acc = balanced_accuracy_score(all_true_labels, all_predictions)\n    average_f1 = f1_score(all_true_labels, all_predictions, average='macro')\n    weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n    f1_scores_per_class = f1_score(all_true_labels, all_predictions, average=None)\n    precision_per_class = precision_score(all_true_labels, all_predictions, average=None, zero_division=0)\n    recall_per_class = recall_score(all_true_labels, all_predictions, average=None, zero_division=0)\n\n    # Append metrics to DataFrame with class labels\n    epoch_metrics = {\n        \"Epoch\": epoch + 1,\n        \"Train Loss\": train_losses[-1],\n        \"Validation Loss\": val_losses[-1],\n        \"Train Accuracy\": train_accuracies[-1],\n        \"Validation Accuracy\": val_accuracies[-1],\n        \"Balanced Accuracy\": balanced_acc,\n        \"Average F1\": average_f1,\n        \"Weighted F1\": weighted_f1\n    }\n    epoch_metrics.update({f\"{label_dict[i]} F1\": f1_scores_per_class[i] for i in range(len(f1_scores_per_class))})\n    epoch_metrics.update({f\"{label_dict[i]} Precision\": precision_per_class[i] for i in range(len(precision_per_class))})\n    epoch_metrics.update({f\"{label_dict[i]} Recall\": recall_per_class[i] for i in range(len(recall_per_class))})\n\n    metrics_df = pd.concat([metrics_df, pd.DataFrame([epoch_metrics])], ignore_index=True)\n\n# Save metrics to CSV and plot accuracies\nmetrics_df.to_csv(f\"{output_dir}/training_metrics.csv\", index=False)\nplot_accuracies(train_accuracies, val_accuracies)\n\n# Print overall model accuracy\noverall_accuracy = accuracy_score(y_test, all_predictions)\nprint(f\"Overall Model Accuracy: {overall_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zip_directory(folder_path, output_path):\n    \"\"\"Zip the contents of an entire directory and save the archive to the specified output path.\"\"\"\n    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(folder_path):\n            for file in files:\n                # Create a relative path for files to preserve the directory structure\n                zipf.write(os.path.join(root, file),\n                           os.path.relpath(os.path.join(root, file),\n                                           os.path.join(folder_path, '..')))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directory to be zipped\ninput_dir = '/kaggle/working/output'\n\n# Output path for the zip file\nzip_file_path = '/kaggle/working/output.zip'\n\n# Creating the ZIP file\nzip_directory(input_dir, zip_file_path)\n\nprint(f\"Created zip file at: {zip_file_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def modify_model(model, num_trainable_layers, dropout_rate):\n    # Freeze layers: Only the last 'num_trainable_layers' are trainable\n    for name, param in model.named_parameters():\n        if 'transformer.layer' in name:\n            layer_num = int(name.split('.')[3])\n            if layer_num < 6 - num_trainable_layers:\n                param.requires_grad = False\n\n    # Adjust dropout rates in transformer layers\n    for layer in model.distilbert.transformer.layer:\n        layer.attention.dropout.p = dropout_rate\n        layer.sa_layer_norm.dropout.p = dropout_rate\n        layer.ffn.dropout.p = dropout_rate\n        layer.output_layer_norm.dropout.p = dropout_rate\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DistilBertConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, device):\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n    average_loss = train_loss / len(train_loader)\n    accuracy = correct_train / total_train\n    return average_loss, accuracy\n\ndef validate_model(model, val_loader, device):\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    all_predictions = []\n    all_true_labels = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_true_labels.extend(batch['labels'].cpu().numpy())\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n    average_val_loss = val_loss / len(val_loader)\n    accuracy = correct_val / total_val\n    return average_val_loss, accuracy, all_predictions, all_true_labels\n\ndef objective(trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n    num_trainable_layers = trial.suggest_int('num_trainable_layers', 1, 6)\n    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n\n    # Model setup and modification\n    model_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n    model = DistilBertForSequenceClassification(model_config)\n    model = modify_model(model, num_trainable_layers, dropout_rate)\n    model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n    # Training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    train_losses = []\n    val_losses = []\n    for epoch in range(2):  # Number of epochs could be part of the trial hyperparameters\n        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, device)\n        val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n        scheduler.step()\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n    # Use any metric or combination for optimization\n    return val_losses[-1]  # Optimize on the last validation loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def modify_model(model, num_trainable_layers, dropout_rate):\n    # Freeze layers: only the last 'num_trainable_layers' are trainable\n    total_layers = len(model.distilbert.transformer.layer)\n    for layer_index, layer in enumerate(model.distilbert.transformer.layer):\n        if layer_index < total_layers - num_trainable_layers:\n            for param in layer.parameters():\n                param.requires_grad = False\n\n    # Adjust dropout rates in applicable transformer layers\n    for layer in model.distilbert.transformer.layer:\n        layer.attention.dropout.p = dropout_rate\n        layer.ffn.dropout.p = dropout_rate\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n\nprint(\"Best trial:\")\nprint(study.best_trial.params)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
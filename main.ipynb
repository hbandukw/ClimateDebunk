{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10542548,"datasetId":6523213,"databundleVersionId":10874335},{"sourceType":"datasetVersion","sourceId":10471355,"datasetId":6483657,"databundleVersionId":10795098}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nlpaug\n!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:57:11.648963Z","iopub.execute_input":"2025-01-24T07:57:11.649300Z","iopub.status.idle":"2025-01-24T07:57:19.578855Z","shell.execute_reply.started":"2025-01-24T07:57:11.649269Z","shell.execute_reply":"2025-01-24T07:57:19.577959Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.12.14)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\nfrom sklearn.metrics import f1_score, confusion_matrix, balanced_accuracy_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.optim import AdamW, lr_scheduler\nimport nlpaug.augmenter.word as naw\nimport optuna\nimport shutil\nimport zipfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:57:19.580253Z","iopub.execute_input":"2025-01-24T07:57:19.580487Z","iopub.status.idle":"2025-01-24T07:58:03.417693Z","shell.execute_reply.started":"2025-01-24T07:57:19.580467Z","shell.execute_reply":"2025-01-24T07:58:03.416951Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Ensure output directory exists\noutput_dir = \"/kaggle/working/output\"\nos.makedirs(output_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:03.419129Z","iopub.execute_input":"2025-01-24T07:58:03.419782Z","iopub.status.idle":"2025-01-24T07:58:03.423750Z","shell.execute_reply.started":"2025-01-24T07:58:03.419757Z","shell.execute_reply":"2025-01-24T07:58:03.422946Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load and prepare data\n# original\ndf = pd.read_parquet(\"/kaggle/input/train-parquet\")\ndf['label_int'] = df['label'].str.split(\"_\").str[0].astype('int')\n\ntexts = df[\"quote\"].to_list()\nlabels = df[\"label_int\"].to_list()\n\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:03.424830Z","iopub.execute_input":"2025-01-24T07:58:03.425166Z","iopub.status.idle":"2025-01-24T07:58:03.905025Z","shell.execute_reply.started":"2025-01-24T07:58:03.425135Z","shell.execute_reply":"2025-01-24T07:58:03.904359Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# augmented \ntrain1 = pd.read_csv('/kaggle/input/balanced/train1.csv')\ntrain2 = pd.read_csv('/kaggle/input/balanced/train2.csv')\ntrain3 = pd.read_csv('/kaggle/input/balanced/train3.csv')\ntrain4 = pd.read_csv('/kaggle/input/balanced/train4.csv')\n\ndatasets = [train1, train2, train3, train4]\n\n# Extract quotes and labels using list comprehension\ntexts = [ds['quote'] for ds in datasets]\nlabels = [ds['numeric_label'] for ds in datasets]\n\n# Concatenate all texts and labels using pandas.concat\ntrain1234_texts = pd.concat(texts, ignore_index=True)\ntrain1234_labels = pd.concat(labels, ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:03.906011Z","iopub.execute_input":"2025-01-24T07:58:03.906399Z","iopub.status.idle":"2025-01-24T07:58:03.922135Z","shell.execute_reply.started":"2025-01-24T07:58:03.906368Z","shell.execute_reply":"2025-01-24T07:58:03.921379Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:03.923046Z","iopub.execute_input":"2025-01-24T07:58:03.923372Z","iopub.status.idle":"2025-01-24T07:58:04.008364Z","shell.execute_reply.started":"2025-01-24T07:58:03.923338Z","shell.execute_reply":"2025-01-24T07:58:04.007213Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Create dictionary for label names\nlabel_dict = df[['label_int', 'label']].drop_duplicates().set_index('label_int')['label'].to_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:04.009455Z","iopub.execute_input":"2025-01-24T07:58:04.009801Z","iopub.status.idle":"2025-01-24T07:58:04.033051Z","shell.execute_reply.started":"2025-01-24T07:58:04.009767Z","shell.execute_reply":"2025-01-24T07:58:04.032350Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\nMAX_LENGTH = 365\n\n# Dataset and DataLoader preparation\nclass QuotesDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ndef encode_data(tokenizer, texts, labels, max_length):\n    try:\n        if isinstance(texts, pd.Series):\n            texts = texts.tolist()\n        if isinstance(labels, pd.Series):\n            labels = labels.tolist()\n            \n        encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        return QuotesDataset(encodings, labels)\n\n    except Exception as e:\n        print(f\"Error during tokenization: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:04.035616Z","iopub.execute_input":"2025-01-24T07:58:04.035929Z","iopub.status.idle":"2025-01-24T07:58:05.383909Z","shell.execute_reply.started":"2025-01-24T07:58:04.035899Z","shell.execute_reply":"2025-01-24T07:58:05.383022Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5047cd2fe1e54b75ae546d6cac99a3af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc27ae6e5a854881b265f1c0917fb37b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e1dc00392bb4966860222cb002f9eee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77bfabfb0607469db56822952f47cb65"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train_dataset = encode_data(tokenizer, X_train, y_train, MAX_LENGTH)\ntrain1234_dataset = encode_data(tokenizer, train1234_texts, train1234_labels, MAX_LENGTH)\nval_dataset = encode_data(tokenizer, X_test, y_test, MAX_LENGTH)\n\n# train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n# train1234_loader = DataLoader(train1234_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:05.396913Z","iopub.execute_input":"2025-01-24T07:58:05.397189Z","iopub.status.idle":"2025-01-24T07:58:27.074091Z","shell.execute_reply.started":"2025-01-24T07:58:05.397168Z","shell.execute_reply":"2025-01-24T07:58:27.073443Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"- Above is the basic data setup. Now train + hyperparam tune the model\n- Start with defining some functions to be used in objective","metadata":{}},{"cell_type":"code","source":"# Changes the layers freezed and drop out rate \ndef modify_model(model, num_trainable_layers, dropout_rate):\n    # Freeze layers: only the last 'num_trainable_layers' are trainable\n    total_layers = len(model.distilbert.transformer.layer)\n    for layer_index, layer in enumerate(model.distilbert.transformer.layer):\n        if layer_index < total_layers - num_trainable_layers:\n            for param in layer.parameters():\n                param.requires_grad = False\n\n    # Adjust dropout rates in applicable transformer layers\n    for layer in model.distilbert.transformer.layer:\n        layer.attention.dropout.p = dropout_rate\n        layer.ffn.dropout.p = dropout_rate\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:27.074803Z","iopub.execute_input":"2025-01-24T07:58:27.075040Z","iopub.status.idle":"2025-01-24T07:58:27.079892Z","shell.execute_reply.started":"2025-01-24T07:58:27.075020Z","shell.execute_reply":"2025-01-24T07:58:27.079010Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Model training \ndef train_one_epoch(model, train_loader, optimizer, device):\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n    average_loss = train_loss / len(train_loader)\n    accuracy = correct_train / total_train\n    return average_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:27.080688Z","iopub.execute_input":"2025-01-24T07:58:27.081017Z","iopub.status.idle":"2025-01-24T07:58:27.114655Z","shell.execute_reply.started":"2025-01-24T07:58:27.080989Z","shell.execute_reply":"2025-01-24T07:58:27.114034Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Model validation \ndef validate_model(model, val_loader, device):\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    all_predictions = []\n    all_true_labels = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_true_labels.extend(batch['labels'].cpu().numpy())\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n    average_val_loss = val_loss / len(val_loader)\n    accuracy = correct_val / total_val\n    return average_val_loss, accuracy, all_predictions, all_true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:27.115379Z","iopub.execute_input":"2025-01-24T07:58:27.115656Z","iopub.status.idle":"2025-01-24T07:58:27.132984Z","shell.execute_reply.started":"2025-01-24T07:58:27.115634Z","shell.execute_reply":"2025-01-24T07:58:27.132167Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Hyperparam tune\ndef objective(trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n    num_trainable_layers = trial.suggest_int('num_trainable_layers', 1, 6)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n    step_size = trial.suggest_int('step_size', 1, 10)\n    gamma = trial.suggest_float('gamma', 0.1, 0.9)\n    epochs = trial.suggest_int('epochs', 2, 5)  # Allowing optimization of number of epochs\n\n    # Model setup and modification\n    model_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n    model = DistilBertForSequenceClassification(model_config)\n    model = modify_model(model, num_trainable_layers, dropout_rate)\n    model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    # Training and validation\n    train_loader = DataLoader(train1234_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    val_accuracies = []\n    for epoch in range(epochs):\n        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, device)\n        val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n        scheduler.step()\n\n        # Collect metrics\n        val_accuracies.append(val_accuracy)\n\n    file_path = f\"/kaggle/working/output_{trial.number}.pth\"\n    torch.save(model.state_dict(), file_path)\n\n    # Store the best or last validation accuracy\n    best_val_accuracy = max(val_accuracies)  # or you could use val_accuracies[-1] for the last\n\n    return best_val_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:27.133738Z","iopub.execute_input":"2025-01-24T07:58:27.133951Z","iopub.status.idle":"2025-01-24T07:58:27.153024Z","shell.execute_reply.started":"2025-01-24T07:58:27.133929Z","shell.execute_reply":"2025-01-24T07:58:27.152248Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)  # Adjust the number of trials as needed\n\nprint(\"Best trial:\")\nprint(study.best_trial.params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:58:27.153832Z","iopub.execute_input":"2025-01-24T07:58:27.154068Z","execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[{"name":"stderr","text":"[I 2025-01-24 07:58:27,168] A new study created in memory with name: no-name-bbcb4c2c-82ec-4197-affb-e21df8ff774f\n<ipython-input-8-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n[I 2025-01-24 08:12:47,827] Trial 0 finished with value: 0.6333059885151764 and parameters: {'learning_rate': 0.0009534888137944198, 'num_trainable_layers': 1, 'dropout_rate': 0.20731170379209174, 'batch_size': 64, 'step_size': 9, 'gamma': 0.6609460758555095, 'epochs': 3}. Best is trial 0 with value: 0.6333059885151764.\n[I 2025-01-24 08:27:56,886] Trial 1 finished with value: 0.8859721082854799 and parameters: {'learning_rate': 0.0001524517384759535, 'num_trainable_layers': 2, 'dropout_rate': 0.355217129984266, 'batch_size': 32, 'step_size': 5, 'gamma': 0.772201387432277, 'epochs': 3}. Best is trial 1 with value: 0.8859721082854799.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"For comparism, run the below on augmented data. ","metadata":{}},{"cell_type":"code","source":"# def objective1(trial):\n#     # Suggest hyperparameters\n#     learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n#     num_trainable_layers = trial.suggest_int('num_trainable_layers', 1, 6)\n#     dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n#     batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n#     step_size = trial.suggest_int('step_size', 1, 10)\n#     gamma = trial.suggest_float('gamma', 0.1, 0.9)\n#     epochs = trial.suggest_int('epochs', 2, 5)  # Allowing optimization of number of epochs\n\n#     # Model setup and modification\n#     model_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n#     model = DistilBertForSequenceClassification(model_config)\n#     model = modify_model(model, num_trainable_layers, dropout_rate)\n#     model.to(device)\n\n#     # Optimizer and scheduler\n#     optimizer = AdamW(model.parameters(), lr=learning_rate)\n#     scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n#     # Training and validation\n#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n#     val_accuracies = []\n#     for epoch in range(epochs):\n#         train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, device)\n#         val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n#         scheduler.step()\n\n#         # Collect metrics\n#         val_accuracies.append(val_accuracy)\n\n#     file_path = f\"/kaggle/working/output1_{trial.number}.pth\"\n#     torch.save(model.state_dict(), file_path)\n\n#     # Store the best or last validation accuracy\n#     best_val_accuracy = max(val_accuracies)  # or you could use val_accuracies[-1] for the last\n\n#     return best_val_accuracy","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# study1 = optuna.create_study(direction='maximize')\n# study1.optimize(objective1, n_trials=50)  # Adjust the number of trials as needed\n\n# print(\"Best trial:\")\n# print(study1.best_trial.params)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the blow code once have best hyperparams ","metadata":{}},{"cell_type":"code","source":"# store this in csv. Try not to re-run the code above. \n# study.best_trial.params","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training params\n# MAX_LENGTH = 365\n\n\n# TRAIN_BATCH_SIZE = 16\n# VAL_BATCH_SIZE = 64\n# LEARNING_RATE = 1e-5\n# STEP_SIZE = 2\n# GAMMA = 0.1\n# EPOCHS = 2","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=8)\n# model.to(device)\n# optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n# scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Plotting function for accuracy\n# def plot_accuracies(training_accuracies, validation_accuracies):\n#     plt.figure(figsize=(8, 6))\n#     plt.plot(training_accuracies, label='Training Accuracy')\n#     plt.plot(validation_accuracies, label='Validation Accuracy')\n#     plt.title('Training and Validation Accuracy')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('Accuracy')\n#     plt.legend()\n#     plt.savefig(f\"{output_dir}/accuracy_plot.png\")\n#     plt.close()\n\n# def plot_confusion_matrix(cm, class_labels, epoch, output_dir):\n#     plt.figure(figsize=(10, 8))\n#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n#     plt.title(f'Confusion Matrix for Epoch {epoch + 1}')\n#     plt.ylabel('True Label')\n#     plt.xlabel('Predicted Label')\n#     plt.savefig(os.path.join(output_dir, f'confusion_matrix_epoch_{epoch + 1}.png'))\n#     plt.close()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Training and validation loop\n# train_losses = []\n# val_losses = []\n# train_accuracies = []\n# val_accuracies = []\n# metrics_df = pd.DataFrame()\n\n# for epoch in range(EPOCHS):\n#     model.train()\n#     train_loss = 0\n#     correct_train = 0\n#     total_train = 0\n\n#     for batch in train_loader:\n#         optimizer.zero_grad()\n#         batch = {k: v.to(device) for k, v in batch.items()}\n#         outputs = model(**batch)\n#         loss = outputs.loss\n#         loss.backward()\n#         optimizer.step()\n#         train_loss += loss.item()\n\n#         predictions = torch.argmax(outputs.logits, dim=-1)\n#         correct_train += (predictions == batch['labels']).sum().item()\n#         total_train += batch['labels'].size(0)\n\n#     scheduler.step()\n#     train_losses.append(train_loss / len(train_loader))\n#     train_accuracies.append(correct_train / total_train)\n\n#     model.eval()\n#     val_loss = 0\n#     correct_val = 0\n#     total_val = 0\n#     all_predictions, all_true_labels = [], []\n\n#     with torch.no_grad():\n#         for batch in val_loader:\n#             batch = {k: v.to(device) for k, v in batch.items()}\n#             outputs = model(**batch)\n#             val_loss += outputs.loss.item()\n#             predictions = torch.argmax(outputs.logits, dim=-1)\n#             all_predictions.extend(predictions.cpu().numpy())\n#             all_true_labels.extend(batch['labels'].cpu().numpy())\n\n#             correct_val += (predictions == batch['labels']).sum().item()\n#             total_val += batch['labels'].size(0)\n\n#     val_losses.append(val_loss / len(val_loader))\n#     val_accuracies.append(correct_val / total_val)\n\n#     # Compute confusion matrix\n#     cm = confusion_matrix(all_true_labels, all_predictions)\n#     class_labels = [label_dict.get(i, f'Class {i}') for i in range(len(np.unique(all_true_labels)))]\n#     plot_confusion_matrix(cm, class_labels, epoch, output_dir)\n\n#     # Calculate metrics\n#     balanced_acc = balanced_accuracy_score(all_true_labels, all_predictions)\n#     average_f1 = f1_score(all_true_labels, all_predictions, average='macro')\n#     weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n#     f1_scores_per_class = f1_score(all_true_labels, all_predictions, average=None)\n#     precision_per_class = precision_score(all_true_labels, all_predictions, average=None, zero_division=0)\n#     recall_per_class = recall_score(all_true_labels, all_predictions, average=None, zero_division=0)\n\n#     # Append metrics to DataFrame with class labels\n#     epoch_metrics = {\n#         \"Epoch\": epoch + 1,\n#         \"Train Loss\": train_losses[-1],\n#         \"Validation Loss\": val_losses[-1],\n#         \"Train Accuracy\": train_accuracies[-1],\n#         \"Validation Accuracy\": val_accuracies[-1],\n#         \"Balanced Accuracy\": balanced_acc,\n#         \"Average F1\": average_f1,\n#         \"Weighted F1\": weighted_f1\n#     }\n#     epoch_metrics.update({f\"{label_dict[i]} F1\": f1_scores_per_class[i] for i in range(len(f1_scores_per_class))})\n#     epoch_metrics.update({f\"{label_dict[i]} Precision\": precision_per_class[i] for i in range(len(precision_per_class))})\n#     epoch_metrics.update({f\"{label_dict[i]} Recall\": recall_per_class[i] for i in range(len(recall_per_class))})\n\n#     metrics_df = pd.concat([metrics_df, pd.DataFrame([epoch_metrics])], ignore_index=True)\n\n# # Save metrics to CSV and plot accuracies\n# metrics_df.to_csv(f\"{output_dir}/training_metrics.csv\", index=False)\n# plot_accuracies(train_accuracies, val_accuracies)\n\n# # Print overall model accuracy\n# overall_accuracy = accuracy_score(y_test, all_predictions)\n# print(f\"Overall Model Accuracy: {overall_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def zip_directory(folder_path, output_path):\n#     \"\"\"Zip the contents of an entire directory and save the archive to the specified output path.\"\"\"\n#     with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n#         for root, dirs, files in os.walk(folder_path):\n#             for file in files:\n#                 # Create a relative path for files to preserve the directory structure\n#                 zipf.write(os.path.join(root, file),\n#                            os.path.relpath(os.path.join(root, file),\n#                                            os.path.join(folder_path, '..')))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Directory to be zipped\n# input_dir = '/kaggle/working/output'\n\n# # Output path for the zip file\n# zip_file_path = '/kaggle/working/output.zip'\n\n# # Creating the ZIP file\n# zip_directory(input_dir, zip_file_path)\n\n# print(f\"Created zip file at: {zip_file_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n\n# print(\"Best trial:\")\n# print(study.best_trial.params)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-24T08:45:56.709Z"}},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10471355,"sourceType":"datasetVersion","datasetId":6483657},{"sourceId":10542548,"sourceType":"datasetVersion","datasetId":6523213},{"sourceId":218658144,"sourceType":"kernelVersion"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nlpaug\n!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:09:32.652962Z","iopub.execute_input":"2025-01-22T17:09:32.653332Z","iopub.status.idle":"2025-01-22T17:09:40.352393Z","shell.execute_reply.started":"2025-01-22T17:09:32.653256Z","shell.execute_reply":"2025-01-22T17:09:40.351567Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.12.14)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport re\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nimport nlpaug.augmenter.word as naw\nimport os\nimport optuna\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:09:40.353882Z","iopub.execute_input":"2025-01-22T17:09:40.354137Z","iopub.status.idle":"2025-01-22T17:10:23.702287Z","shell.execute_reply.started":"2025-01-22T17:09:40.354113Z","shell.execute_reply":"2025-01-22T17:10:23.701664Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"- source of data: https://huggingface.co/datasets/QuotaClimat/frugalaichallenge-text-train","metadata":{}},{"cell_type":"code","source":"train1 = pd.read_csv('/kaggle/input/d/rafechang/balanced/train1.csv')\ntrain2 = pd.read_csv('/kaggle/input/d/rafechang/balanced/train2.csv')\ntrain3 = pd.read_csv('/kaggle/input/d/rafechang/balanced/train3.csv')\ntrain4 = pd.read_csv('/kaggle/input/d/rafechang/balanced/train4.csv')\ntest = pd.read_csv('/kaggle/input/d/rafechang/balanced/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:10:23.703432Z","iopub.execute_input":"2025-01-22T17:10:23.704036Z","iopub.status.idle":"2025-01-22T17:10:23.857836Z","shell.execute_reply.started":"2025-01-22T17:10:23.704011Z","shell.execute_reply":"2025-01-22T17:10:23.856941Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:10:23.859109Z","iopub.execute_input":"2025-01-22T17:10:23.859321Z","iopub.status.idle":"2025-01-22T17:10:23.936443Z","shell.execute_reply.started":"2025-01-22T17:10:23.859303Z","shell.execute_reply":"2025-01-22T17:10:23.935654Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"- Distilbert should be less energy consuming, it has less params \n- Lower case so less params ","metadata":{}},{"cell_type":"markdown","source":"**split data**","metadata":{}},{"cell_type":"code","source":"train1_texts = train1['quote']\ntrain1_labels = train1['numeric_label']\ntrain2_texts = train2['quote']\ntrain2_labels = train2['numeric_label']\ntrain3_texts = train3['quote']\ntrain3_labels = train3['numeric_label']\ntrain4_texts = train4['quote']\ntrain4_labels = train4['numeric_label']\n\ntrain12_texts = pd.concat([train1_texts, train2_texts], ignore_index=True)\ntrain12_labels = pd.concat([train1_labels, train2_labels], ignore_index=True)\ntrain123_texts = pd.concat([train12_texts, train3_texts], ignore_index=True)\ntrain123_labels = pd.concat([train12_labels, train3_labels], ignore_index=True)\n\ntest_texts = test['quote']\ntest_labels = test['numeric_label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:10:23.937216Z","iopub.execute_input":"2025-01-22T17:10:23.937442Z","iopub.status.idle":"2025-01-22T17:10:23.953378Z","shell.execute_reply.started":"2025-01-22T17:10:23.937409Z","shell.execute_reply":"2025-01-22T17:10:23.952672Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**Tokenize** ","metadata":{}},{"cell_type":"code","source":"# Initialize the BERT tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n\n# Function to tokenize data\ndef tokenize_data(texts, labels):\n    try:\n        if isinstance(texts, pd.Series):\n            texts = texts.tolist()\n        if isinstance(labels, pd.Series):\n            labels = labels.tolist()\n\n        encodings = tokenizer(\n            texts, \n            padding=True, \n            truncation=True, \n            max_length=367, \n            return_tensors=\"pt\"\n        )\n\n        dataset = CustomTextDataset(encodings, labels)\n        return dataset\n\n    except Exception as e:\n        print(f\"Error during tokenization: {e}\")\n        return None\n# Custom Dataset class\nclass CustomTextDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = [int(label) for label in labels]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:10:23.954108Z","iopub.execute_input":"2025-01-22T17:10:23.954323Z","iopub.status.idle":"2025-01-22T17:10:26.095405Z","shell.execute_reply.started":"2025-01-22T17:10:23.954305Z","shell.execute_reply":"2025-01-22T17:10:26.094797Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"988b4387817240559da46361306cbc45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e25c049687844e438bf125bf2962cece"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36eee17d79a47fca997a05b50b91e43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f0dd5c1f9445f3bde1da2b6b5828de"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train1_dataset = tokenize_data(train1_texts, train1_labels)\ntrain2_dataset = tokenize_data(train2_texts, train2_labels)\ntrain3_dataset = tokenize_data(train3_texts, train3_labels)\ntrain4_dataset = tokenize_data(train4_texts, train4_labels)\ntrain12_dataset = tokenize_data(train12_texts, train12_labels)\ntrain123_dataset = tokenize_data(train123_texts, train123_labels)\n\ntest_dataset = tokenize_data(test_texts, test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:10:26.096155Z","iopub.execute_input":"2025-01-22T17:10:26.096460Z","iopub.status.idle":"2025-01-22T17:10:58.119857Z","shell.execute_reply.started":"2025-01-22T17:10:26.096425Z","shell.execute_reply":"2025-01-22T17:10:58.119170Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train1_loader = DataLoader(train1_dataset, batch_size=32, shuffle=True)\ntrain2_loader = DataLoader(train2_dataset, batch_size=32, shuffle=True)\ntrain3_loader = DataLoader(train3_dataset, batch_size=32, shuffle=True)\ntrain4_loader = DataLoader(train4_dataset, batch_size=32, shuffle=True)\ntrain12_loader = DataLoader(train12_dataset, batch_size=32, shuffle=True)\ntrain123_loader = DataLoader(train123_dataset, batch_size=32, shuffle=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:10:58.122171Z","iopub.execute_input":"2025-01-22T17:10:58.122386Z","iopub.status.idle":"2025-01-22T17:10:58.127134Z","shell.execute_reply.started":"2025-01-22T17:10:58.122367Z","shell.execute_reply":"2025-01-22T17:10:58.126335Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model1 = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 8)\nmodel1.to(device)\noptimizer1 = AdamW(model1.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:10:58.128513Z","iopub.execute_input":"2025-01-22T17:10:58.128815Z","iopub.status.idle":"2025-01-22T17:11:00.217331Z","shell.execute_reply.started":"2025-01-22T17:10:58.128785Z","shell.execute_reply":"2025-01-22T17:11:00.216542Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8506505eb47d4391ba4a091aa4be6b07"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def train_and_evaluate_model(model, train_loader, val_loader, optimizer, device, num_epochs=2):\n    \"\"\"\n    Trains a given model for a specified number of epochs and evaluates it on a validation set,\n    printing out the loss, accuracy, and F1 score.\n\n    Parameters:\n    - model: the PyTorch model to be trained.\n    - train_loader: DataLoader for the training data.\n    - val_loader: DataLoader for the validation data.\n    - optimizer: the optimizer used to update the model's weights.\n    - device: the device (e.g., 'cuda' or 'cpu') to perform training on.\n    - num_epochs: the number of epochs to train for (default is 2).\n    \"\"\"\n    model.to(device)\n\n    for epoch in range(num_epochs):  # Loop over the dataset multiple times\n        model.train()  # Set the model to training mode\n        total_loss = 0\n        total_correct = 0\n        total_examples = 0\n\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()  # Clear gradients\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()  # Backpropagation\n            optimizer.step()  # Update parameters\n\n            total_loss += loss.item()\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            total_correct += (predictions == batch['labels']).sum().item()\n            total_examples += predictions.size(0)\n\n        avg_loss = total_loss / len(train_loader)\n        avg_accuracy = 100 * total_correct / total_examples\n        print(f\"Epoch {epoch + 1}, Training Loss: {avg_loss:.2f}, Training Accuracy: {avg_accuracy:.2f}%\")\n\n        # Validation loop\n        model.eval()  # Set the model to evaluation mode\n        val_total_loss = 0\n        val_total_correct = 0\n        val_total_examples = 0\n        val_predictions = []\n        val_true_labels = []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(**batch)\n                val_loss = outputs.loss\n                val_total_loss += val_loss.item()\n\n                logits = outputs.logits\n                predictions = torch.argmax(logits, dim=-1)\n                val_total_correct += (predictions == batch['labels']).sum().item()\n                val_total_examples += predictions.size(0)\n\n                val_predictions.extend(predictions.cpu().numpy())\n                val_true_labels.extend(batch['labels'].cpu().numpy())\n\n        val_avg_loss = val_total_loss / len(val_loader)\n        val_avg_accuracy = 100 * val_total_correct / val_total_examples\n        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n\n        print(f\"Epoch {epoch + 1}, Validation Loss: {val_avg_loss:.2f}, Validation Accuracy: {val_avg_accuracy:.2f}%, F1 Score: {val_f1:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:11:00.218165Z","iopub.execute_input":"2025-01-22T17:11:00.218497Z","iopub.status.idle":"2025-01-22T17:11:00.226742Z","shell.execute_reply.started":"2025-01-22T17:11:00.218466Z","shell.execute_reply":"2025-01-22T17:11:00.225917Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Step 1: train on train1 and validate on train 2 ","metadata":{}},{"cell_type":"code","source":"train_and_evaluate_model(model1, train1_loader, train2_loader, optimizer1, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:11:00.227623Z","iopub.execute_input":"2025-01-22T17:11:00.227899Z","iopub.status.idle":"2025-01-22T17:14:15.054740Z","shell.execute_reply.started":"2025-01-22T17:11:00.227872Z","shell.execute_reply":"2025-01-22T17:14:15.053745Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 1.52, Training Accuracy: 47.70%\nEpoch 1, Validation Loss: 1.21, Validation Accuracy: 57.73%, F1 Score: 0.57\nEpoch 2, Training Loss: 0.69, Training Accuracy: 79.61%\nEpoch 2, Validation Loss: 1.29, Validation Accuracy: 55.04%, F1 Score: 0.55\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Step 2: train on train1+train 2, validate on train 3","metadata":{}},{"cell_type":"code","source":"train_and_evaluate_model(model1, train12_loader, train3_loader, optimizer1, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:14:15.055749Z","iopub.execute_input":"2025-01-22T17:14:15.056070Z","iopub.status.idle":"2025-01-22T17:20:20.597976Z","shell.execute_reply.started":"2025-01-22T17:14:15.056034Z","shell.execute_reply":"2025-01-22T17:20:20.597155Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.68, Training Accuracy: 78.39%\nEpoch 1, Validation Loss: 1.11, Validation Accuracy: 62.20%, F1 Score: 0.62\nEpoch 2, Training Loss: 0.24, Training Accuracy: 93.30%\nEpoch 2, Validation Loss: 1.41, Validation Accuracy: 60.94%, F1 Score: 0.61\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Step 3: train on train1+train2+train3, validate on train4 ","metadata":{}},{"cell_type":"code","source":"train_and_evaluate_model(model1, train123_loader, train4_loader, optimizer1, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:20:20.598981Z","iopub.execute_input":"2025-01-22T17:20:20.599298Z","iopub.status.idle":"2025-01-22T17:29:15.627816Z","shell.execute_reply.started":"2025-01-22T17:20:20.599272Z","shell.execute_reply":"2025-01-22T17:29:15.626952Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.42, Training Accuracy: 87.57%\nEpoch 1, Validation Loss: 1.36, Validation Accuracy: 60.45%, F1 Score: 0.61\nEpoch 2, Training Loss: 0.13, Training Accuracy: 96.51%\nEpoch 2, Validation Loss: 1.69, Validation Accuracy: 60.93%, F1 Score: 0.61\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Before hyperparam optimization, I am trying to compare the above method vs training everything at once ","metadata":{}},{"cell_type":"code","source":"model2 = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 8)\nmodel2.to(device)\noptimizer2 = AdamW(model2.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:29:15.628872Z","iopub.execute_input":"2025-01-22T17:29:15.629105Z","iopub.status.idle":"2025-01-22T17:29:16.008270Z","shell.execute_reply.started":"2025-01-22T17:29:15.629084Z","shell.execute_reply":"2025-01-22T17:29:16.007330Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Step 4: hyperparam optimization (only here since can be resource intensive); use train4 for validation ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # Hyperparameters to tune\n    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n    batch_size = trial.suggest_categorical('batch_size', [4, 8, 16])\n    num_trainable_layers = trial.suggest_int('num_trainable_layers', 1, 6)\n    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n\n    # Call the training function\n    loss = train_model(lr, batch_size, num_trainable_layers, dropout_rate, dataset)\n\n    return loss  # Objective to minimize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:29:16.008931Z","iopub.execute_input":"2025-01-22T17:29:16.009161Z","iopub.status.idle":"2025-01-22T17:29:16.013504Z","shell.execute_reply.started":"2025-01-22T17:29:16.009140Z","shell.execute_reply":"2025-01-22T17:29:16.012612Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"havnt done early stopping ","metadata":{}}]}
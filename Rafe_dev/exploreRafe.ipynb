{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10471355,"sourceType":"datasetVersion","datasetId":6483657},{"sourceId":10530226,"sourceType":"datasetVersion","datasetId":6516691},{"sourceId":218492595,"sourceType":"kernelVersion"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install nlpaug","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:50:59.828819Z","iopub.execute_input":"2025-01-20T19:50:59.829078Z","iopub.status.idle":"2025-01-20T19:51:05.979556Z","shell.execute_reply.started":"2025-01-20T19:50:59.829056Z","shell.execute_reply":"2025-01-20T19:51:05.978694Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.12.14)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport re\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nimport nlpaug.augmenter.word as naw\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:51:09.152256Z","iopub.execute_input":"2025-01-20T19:51:09.152549Z","iopub.status.idle":"2025-01-20T19:52:07.343558Z","shell.execute_reply.started":"2025-01-20T19:51:09.152523Z","shell.execute_reply":"2025-01-20T19:52:07.343021Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"- source of data: https://huggingface.co/datasets/QuotaClimat/frugalaichallenge-text-train","metadata":{}},{"cell_type":"code","source":"train1 = pd.read_csv('/kaggle/input/balanced/train1.csv')\ntrain2 = pd.read_csv('/kaggle/input/balanced/train2.csv')\ntrain3 = pd.read_csv('/kaggle/input/balanced/train3.csv')\ntrain4 = pd.read_csv('/kaggle/input/balanced/train4.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:05:42.403473Z","iopub.execute_input":"2025-01-20T20:05:42.403822Z","iopub.status.idle":"2025-01-20T20:05:42.579031Z","shell.execute_reply.started":"2025-01-20T20:05:42.403796Z","shell.execute_reply":"2025-01-20T20:05:42.578354Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:56:35.541853Z","iopub.execute_input":"2025-01-20T19:56:35.542254Z","iopub.status.idle":"2025-01-20T19:56:35.629311Z","shell.execute_reply.started":"2025-01-20T19:56:35.542227Z","shell.execute_reply":"2025-01-20T19:56:35.628452Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"- Distilbert should be less energy consuming, it has less params \n- Lower case so less params ","metadata":{}},{"cell_type":"markdown","source":"**split data**","metadata":{}},{"cell_type":"code","source":"train1_texts = train1['quote']\ntrain1_labels = train1['numeric_label']\ntrain2_texts = train2['quote']\ntrain2_labels = train2['numeric_label']\ntrain3_texts = train3['quote']\ntrain3_labels = train3['numeric_label']\ntrain4_texts = train4['quote']\ntrain4_labels = train4['numeric_label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:06:28.760289Z","iopub.execute_input":"2025-01-20T20:06:28.760611Z","iopub.status.idle":"2025-01-20T20:06:28.765615Z","shell.execute_reply.started":"2025-01-20T20:06:28.760588Z","shell.execute_reply":"2025-01-20T20:06:28.764748Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Tokenize** ","metadata":{}},{"cell_type":"code","source":"# Initialize the BERT tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n\n# Function to tokenize data\ndef tokenize_data(texts, labels):\n    try:\n        if isinstance(texts, pd.Series):\n            texts = texts.tolist()\n        if isinstance(labels, pd.Series):\n            labels = labels.tolist()\n\n        encodings = tokenizer(\n            texts, \n            padding=True, \n            truncation=True, \n            max_length=367, \n            return_tensors=\"pt\"\n        )\n\n        dataset = CustomTextDataset(encodings, labels)\n        return dataset\n\n    except Exception as e:\n        print(f\"Error during tokenization: {e}\")\n        return None\n# Custom Dataset class\nclass CustomTextDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = [int(label) for label in labels]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:06:36.403628Z","iopub.execute_input":"2025-01-20T20:06:36.403973Z","iopub.status.idle":"2025-01-20T20:06:37.167049Z","shell.execute_reply.started":"2025-01-20T20:06:36.403949Z","shell.execute_reply":"2025-01-20T20:06:37.166202Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309d821e40504f639af2705e35ecae4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd30bc10cae48c7a9b1064b73d5ad1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dde3be28d0246a08f56b11e02cadaf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bdca6a580f249f7aa9c320284f25184"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train1_dataset = tokenize_data(train1_texts, train1_labels)\ntrain2_dataset = tokenize_data(train2_texts, train2_labels)\ntrain3_dataset = tokenize_data(train3_texts, train3_labels)\ntrain4_dataset = tokenize_data(train4_texts, train4_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:07:19.247847Z","iopub.execute_input":"2025-01-20T20:07:19.248194Z","iopub.status.idle":"2025-01-20T20:07:33.500101Z","shell.execute_reply.started":"2025-01-20T20:07:19.248166Z","shell.execute_reply":"2025-01-20T20:07:33.499169Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train1_loader = DataLoader(train1_dataset, batch_size=32, shuffle=True)\ntrain2_loader = DataLoader(train2_dataset, batch_size=32, shuffle=True)\ntrain3_loader = DataLoader(train3_dataset, batch_size=32, shuffle=True)\ntrain4_loader = DataLoader(train4_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:26:58.942115Z","iopub.execute_input":"2025-01-20T20:26:58.942444Z","iopub.status.idle":"2025-01-20T20:26:58.946861Z","shell.execute_reply.started":"2025-01-20T20:26:58.942417Z","shell.execute_reply":"2025-01-20T20:26:58.945965Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model1 = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 8)\nmodel1.to(device)\noptimizer1 = AdamW(model1.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:43:26.634580Z","iopub.execute_input":"2025-01-20T20:43:26.634943Z","iopub.status.idle":"2025-01-20T20:43:27.034949Z","shell.execute_reply.started":"2025-01-20T20:43:26.634900Z","shell.execute_reply":"2025-01-20T20:43:27.034089Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:08:42.339724Z","iopub.execute_input":"2025-01-20T20:08:42.340202Z","iopub.status.idle":"2025-01-20T20:08:42.345496Z","shell.execute_reply.started":"2025-01-20T20:08:42.340161Z","shell.execute_reply":"2025-01-20T20:08:42.344423Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Step 1: train on train1 and validate on train 2 ","metadata":{}},{"cell_type":"code","source":"model1.train()  # Set the model to training mode\n\nfor epoch in range(2):  # Train for 4 epochs\n    total_loss = 0\n    total_correct = 0\n    total_examples = 0\n\n    for batch in train1_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch to device\n        outputs = model1(**batch)  # Forward pass\n        loss = outputs.loss\n        loss.backward()  # Backpropagation\n        optimizer1.step()  # Update parameters\n        optimizer1.zero_grad()  # Clear gradients\n\n        # Calculate the loss\n        total_loss += loss.item()\n\n        # Calculate accuracy\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        total_correct += (predictions == batch['labels']).sum().item()\n        total_examples += predictions.size(0)\n\n    # Calculate average loss and accuracy for the epoch\n    avg_loss = total_loss / len(train1_loader)\n    avg_accuracy = 100 * total_correct / total_examples\n\n    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.2f}, Accuracy: {avg_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:43:30.314206Z","iopub.execute_input":"2025-01-20T20:43:30.314531Z","iopub.status.idle":"2025-01-20T20:46:08.163347Z","shell.execute_reply.started":"2025-01-20T20:43:30.314504Z","shell.execute_reply":"2025-01-20T20:46:08.162581Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.52, Accuracy: 47.01%\nEpoch 2, Loss: 0.71, Accuracy: 78.58%\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"model1.eval()\ntotal1_eval_accuracy = 0\ntotal1_eval_loss = 0\n\nfor batch in train2_loader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model1(**batch)\n\n    logits = outputs.logits\n    loss = outputs.loss\n    total1_eval_loss += loss.item()\n\n    predictions = torch.argmax(logits, dim=-1)\n    accuracy = (predictions == batch['labels']).cpu().numpy().mean() * 100\n    total1_eval_accuracy += accuracy\n\navg1_test_accuracy = total1_eval_accuracy / len(train2_loader)\navg1_test_loss = total1_eval_loss / len(train2_loader)\n\nprint(f\"Test Loss: {avg1_test_loss}, Test Accuracy: {avg1_test_accuracy}\")\n\npredictions1, true_labels1 = [], []\n\nfor batch in train2_loader:\n    # Move batch to the appropriate device\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model1(**batch)\n\n    logits = outputs.logits\n    pred_labels = torch.argmax(logits, dim=-1)\n\n    # Collect predictions and true labels\n    predictions1.extend(pred_labels.cpu().numpy())\n    true_labels1.extend(batch['labels'].cpu().numpy())\n\nf1 = f1_score(true_labels1, predictions1, average='weighted')  # Change 'weighted' to 'macro' if needed\n\nprint(f\"F1 Score: {f1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:46:18.687945Z","iopub.execute_input":"2025-01-20T20:46:18.688244Z","iopub.status.idle":"2025-01-20T20:47:18.813605Z","shell.execute_reply.started":"2025-01-20T20:46:18.688222Z","shell.execute_reply":"2025-01-20T20:47:18.812777Z"}},"outputs":[{"name":"stdout","text":"Test Loss: 1.3220378246652074, Test Accuracy: 54.66867469879518\nF1 Score: 0.5439709223591096\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"Step 2: train on train1+train 2, validate on train 3","metadata":{}},{"cell_type":"code","source":"train12_texts = pd.concat([train1_texts, train2_texts], ignore_index=True)\ntrain12_labels = pd.concat([train1_labels, train2_labels], ignore_index=True)\ntrain12_dataset = tokenize_data(train12_texts, train12_labels)\ntrain12_loader = DataLoader(train12_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:49:17.949078Z","iopub.execute_input":"2025-01-20T20:49:17.949402Z","iopub.status.idle":"2025-01-20T20:49:24.579401Z","shell.execute_reply.started":"2025-01-20T20:49:17.949380Z","shell.execute_reply":"2025-01-20T20:49:24.578709Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model1.train()  # Set the model to training mode\n\nfor epoch in range(2):  # Train for 4 epochs\n    total_loss = 0\n    total_correct = 0\n    total_examples = 0\n\n    for batch in train12_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch to device\n        outputs = model1(**batch)  # Forward pass\n        loss = outputs.loss\n        loss.backward()  # Backpropagation\n        optimizer1.step()  # Update parameters\n        optimizer1.zero_grad()  # Clear gradients\n\n        # Calculate the loss\n        total_loss += loss.item()\n\n        # Calculate accuracy\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        total_correct += (predictions == batch['labels']).sum().item()\n        total_examples += predictions.size(0)\n\n    # Calculate average loss and accuracy for the epoch\n    avg_loss = total_loss / len(train12_loader)\n    avg_accuracy = 100 * total_correct / total_examples\n\n    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.2f}, Accuracy: {avg_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T20:50:02.084199Z","iopub.execute_input":"2025-01-20T20:50:02.084514Z","iopub.status.idle":"2025-01-20T20:55:37.514849Z","shell.execute_reply.started":"2025-01-20T20:50:02.084491Z","shell.execute_reply":"2025-01-20T20:55:37.514003Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.68, Accuracy: 78.05%\nEpoch 2, Loss: 0.26, Accuracy: 92.11%\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"model1.eval()\ntotal2_eval_accuracy = 0\ntotal2_eval_loss = 0\n\nfor batch in train3_loader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model1(**batch)\n\n    logits = outputs.logits\n    loss = outputs.loss\n    total2_eval_loss += loss.item()\n\n    predictions = torch.argmax(logits, dim=-1)\n    accuracy = (predictions == batch['labels']).cpu().numpy().mean() * 100\n    total2_eval_accuracy += accuracy\n\navg2_test_accuracy = total2_eval_accuracy / len(train3_loader)\navg2_test_loss = total2_eval_loss / len(train3_loader)\n\nprint(f\"Test Loss: {avg2_test_loss}, Test Accuracy: {avg2_test_accuracy}\")\n\npredictions2, true_labels2 = [], []\n\nfor batch in train3_loader:\n    # Move batch to the appropriate device\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model1(**batch)\n\n    logits = outputs.logits\n    pred_labels = torch.argmax(logits, dim=-1)\n\n    # Collect predictions and true labels\n    predictions2.extend(pred_labels.cpu().numpy())\n    true_labels2.extend(batch['labels'].cpu().numpy())\n\nf12 = f1_score(true_labels2, predictions2, average='weighted')  # Change 'weighted' to 'macro' if needed\n\nprint(f\"F1 Score: {f12}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T21:02:55.073682Z","iopub.execute_input":"2025-01-20T21:02:55.074054Z","iopub.status.idle":"2025-01-20T21:03:54.358766Z","shell.execute_reply.started":"2025-01-20T21:02:55.074027Z","shell.execute_reply":"2025-01-20T21:03:54.357827Z"}},"outputs":[{"name":"stdout","text":"Test Loss: 0.9829255719500852, Test Accuracy: 73.26807228915662\nF1 Score: 0.733328190754836\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Step 3: train on train1+train2+train3, validate on train4 ","metadata":{}},{"cell_type":"markdown","source":"Step 4: hyperparam optimization (only here since can be resource intensive) ","metadata":{}}]}
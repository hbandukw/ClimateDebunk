{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10471355,"sourceType":"datasetVersion","datasetId":6483657},{"sourceId":10542548,"sourceType":"datasetVersion","datasetId":6523213}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Below is the best hyperparamter resulted in the `main.ipynb` optimization. The purpose of this notebook is to see the effectiveness of the optimization. \n\n[I 2025-01-24 19:55:40,455] Trial 27 finished with value: 0.904019688269073 and parameters: {'learning_rate': 8.300432875328772e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.3847406475130443, 'batch_size': 32, 'step_size': 9, 'gamma': 0.5951936405857416, 'epochs': 5}. Best is trial 27 with value: 0.904019688269073.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\nfrom sklearn.metrics import f1_score, confusion_matrix, balanced_accuracy_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.optim import AdamW, lr_scheduler\nimport shutil\nimport zipfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:07.054632Z","iopub.execute_input":"2025-01-26T19:57:07.055009Z","iopub.status.idle":"2025-01-26T19:57:24.936637Z","shell.execute_reply.started":"2025-01-26T19:57:07.054978Z","shell.execute_reply":"2025-01-26T19:57:24.935976Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"learning_rate = 8.300432875328772e-05\nnum_trainable_layers = 2\ndropout_rate = 0.3847406475130443\nbatch_size = 32\nstep_size = 9\ngamma = 0.5951936405857416\nepochs = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:24.937551Z","iopub.execute_input":"2025-01-26T19:57:24.937981Z","iopub.status.idle":"2025-01-26T19:57:24.941876Z","shell.execute_reply.started":"2025-01-26T19:57:24.937941Z","shell.execute_reply":"2025-01-26T19:57:24.941027Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:24.943828Z","iopub.execute_input":"2025-01-26T19:57:24.944110Z","iopub.status.idle":"2025-01-26T19:57:25.044158Z","shell.execute_reply.started":"2025-01-26T19:57:24.944082Z","shell.execute_reply":"2025-01-26T19:57:25.043249Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load and prepare data\n# original\ndf = pd.read_parquet(\"/kaggle/input/train-parquet\")\ndf['label_int'] = df['label'].str.split(\"_\").str[0].astype('int')\n\ntexts = df[\"quote\"].to_list()\nlabels = df[\"label_int\"].to_list()\n\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:25.045385Z","iopub.execute_input":"2025-01-26T19:57:25.045718Z","iopub.status.idle":"2025-01-26T19:57:25.249868Z","shell.execute_reply.started":"2025-01-26T19:57:25.045691Z","shell.execute_reply":"2025-01-26T19:57:25.249190Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# aug \ntrain1 = pd.read_csv('/kaggle/input/balanced/train1.csv')\ntrain2 = pd.read_csv('/kaggle/input/balanced/train2.csv')\ntrain3 = pd.read_csv('/kaggle/input/balanced/train3.csv')\ntrain4 = pd.read_csv('/kaggle/input/balanced/train4.csv')\n\ndatasets = [train1, train2, train3, train4]\n\n# Extract quotes and labels using list comprehension\ntexts = [ds['quote'] for ds in datasets]\nlabels = [ds['numeric_label'] for ds in datasets]\n\n# Concatenate all texts and labels using pandas.concat\ntrain1234_texts = pd.concat(texts, ignore_index=True)\ntrain1234_labels = pd.concat(labels, ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:25.250733Z","iopub.execute_input":"2025-01-26T19:57:25.251029Z","iopub.status.idle":"2025-01-26T19:57:25.387277Z","shell.execute_reply.started":"2025-01-26T19:57:25.250998Z","shell.execute_reply":"2025-01-26T19:57:25.386631Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\nMAX_LENGTH = 365\n\n# Dataset and DataLoader preparation\nclass QuotesDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ndef encode_data(tokenizer, texts, labels, max_length):\n    try:\n        if isinstance(texts, pd.Series):\n            texts = texts.tolist()\n        if isinstance(labels, pd.Series):\n            labels = labels.tolist()\n            \n        encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        return QuotesDataset(encodings, labels)\n\n    except Exception as e:\n        print(f\"Error during tokenization: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:25.388039Z","iopub.execute_input":"2025-01-26T19:57:25.388267Z","iopub.status.idle":"2025-01-26T19:57:26.289569Z","shell.execute_reply.started":"2025-01-26T19:57:25.388247Z","shell.execute_reply":"2025-01-26T19:57:26.288899Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8fc16d2c01547e288d7ea9931dfbc03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8da7434bef40b698a9097ebb0d9017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43e97817065a4e608bb4b13a177167c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0423419ad9e4464ebc33fb6414ab5e50"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train1234_dataset = encode_data(tokenizer, train1234_texts, train1234_labels, MAX_LENGTH)\nval_dataset = encode_data(tokenizer, X_test, y_test, MAX_LENGTH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:26.290351Z","iopub.execute_input":"2025-01-26T19:57:26.290603Z","iopub.status.idle":"2025-01-26T19:57:41.483683Z","shell.execute_reply.started":"2025-01-26T19:57:26.290582Z","shell.execute_reply":"2025-01-26T19:57:41.482962Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def modify_model(model, num_trainable_layers, dropout_rate):\n    # Freeze layers: only the last 'num_trainable_layers' are trainable\n    total_layers = len(model.distilbert.transformer.layer)\n    for layer_index, layer in enumerate(model.distilbert.transformer.layer):\n        if layer_index < total_layers - num_trainable_layers:\n            for param in layer.parameters():\n                param.requires_grad = False\n\n    # Adjust dropout rates in applicable transformer layers\n    for layer in model.distilbert.transformer.layer:\n        layer.attention.dropout.p = dropout_rate\n        layer.ffn.dropout.p = dropout_rate\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:41.484523Z","iopub.execute_input":"2025-01-26T19:57:41.484762Z","iopub.status.idle":"2025-01-26T19:57:41.489202Z","shell.execute_reply.started":"2025-01-26T19:57:41.484740Z","shell.execute_reply":"2025-01-26T19:57:41.488364Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, device):\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n    average_loss = train_loss / len(train_loader)\n    accuracy = correct_train / total_train\n    return average_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:41.491283Z","iopub.execute_input":"2025-01-26T19:57:41.491523Z","iopub.status.idle":"2025-01-26T19:57:41.502269Z","shell.execute_reply.started":"2025-01-26T19:57:41.491504Z","shell.execute_reply":"2025-01-26T19:57:41.501599Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def validate_model(model, val_loader, device):\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    all_predictions = []\n    all_true_labels = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_true_labels.extend(batch['labels'].cpu().numpy())\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n    average_val_loss = val_loss / len(val_loader)\n    accuracy = correct_val / total_val\n    return average_val_loss, accuracy, all_predictions, all_true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:41.503523Z","iopub.execute_input":"2025-01-26T19:57:41.503743Z","iopub.status.idle":"2025-01-26T19:57:41.516467Z","shell.execute_reply.started":"2025-01-26T19:57:41.503724Z","shell.execute_reply":"2025-01-26T19:57:41.515639Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\nmodel = DistilBertForSequenceClassification(model_config)\nmodel = modify_model(model, num_trainable_layers, dropout_rate)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:41.517293Z","iopub.execute_input":"2025-01-26T19:57:41.517551Z","iopub.status.idle":"2025-01-26T19:57:42.898570Z","shell.execute_reply.started":"2025-01-26T19:57:41.517522Z","shell.execute_reply":"2025-01-26T19:57:42.897810Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.3847406475130443, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.3847406475130443, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=8, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr= learning_rate)\nscheduler = lr_scheduler.StepLR(optimizer, step_size= step_size, gamma= gamma)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:42.899293Z","iopub.execute_input":"2025-01-26T19:57:42.899564Z","iopub.status.idle":"2025-01-26T19:57:42.903994Z","shell.execute_reply.started":"2025-01-26T19:57:42.899530Z","shell.execute_reply":"2025-01-26T19:57:42.903133Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_loader = DataLoader(train1234_dataset, batch_size= batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size= batch_size, shuffle=False) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:42.904910Z","iopub.execute_input":"2025-01-26T19:57:42.905204Z","iopub.status.idle":"2025-01-26T19:57:42.915658Z","shell.execute_reply.started":"2025-01-26T19:57:42.905174Z","shell.execute_reply":"2025-01-26T19:57:42.914850Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"val_accuracies = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:42.916446Z","iopub.execute_input":"2025-01-26T19:57:42.916682Z","iopub.status.idle":"2025-01-26T19:57:42.929236Z","shell.execute_reply.started":"2025-01-26T19:57:42.916663Z","shell.execute_reply":"2025-01-26T19:57:42.928620Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"for epoch in range(epochs):\n    train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, device)\n    val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n    scheduler.step()\n\n    val_accuracies.append(val_accuracy)\n    print(val_accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:57:42.929874Z","iopub.execute_input":"2025-01-26T19:57:42.930151Z","iopub.status.idle":"2025-01-26T20:21:38.853005Z","shell.execute_reply.started":"2025-01-26T19:57:42.930131Z","shell.execute_reply":"2025-01-26T20:21:38.852139Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"name":"stdout","text":"0.579163248564397\n0.7653814602132896\n0.844954881050041\n0.8843314191960624\n0.896636587366694\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"val_accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:21:38.853905Z","iopub.execute_input":"2025-01-26T20:21:38.854188Z","iopub.status.idle":"2025-01-26T20:21:38.859548Z","shell.execute_reply.started":"2025-01-26T20:21:38.854153Z","shell.execute_reply":"2025-01-26T20:21:38.858798Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[0.579163248564397,\n 0.7653814602132896,\n 0.844954881050041,\n 0.8843314191960624,\n 0.896636587366694]"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"Next step is to deciede what data to be used for the final model. ","metadata":{}},{"cell_type":"markdown","source":"For document purpose, below is the log of the log of `main.ipynb` that got interrupted because of time out (completed total of 30 trials instead of intended 40) \n\nTime\n\nLog Message\n10.9s\t1\tCollecting nlpaug\n10.9s\t2\t  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n10.9s\t3\tRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\n10.9s\t4\tRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\n11.0s\t5\tRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\n11.0s\t6\tRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\n11.0s\t7\tRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n11.0s\t8\tRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\n11.0s\t9\tRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n11.0s\t10\tRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\n11.0s\t11\tRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\n11.0s\t12\tRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\n11.0s\t13\tRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2025.0.1)\n11.0s\t14\tRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2022.0.0)\n11.0s\t15\tRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\n11.0s\t16\tRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n11.0s\t17\tRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n11.0s\t18\tRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n11.0s\t19\tRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\n11.0s\t20\tRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n11.0s\t21\tRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\n11.0s\t22\tRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.12.14)\n11.0s\t23\tRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n11.0s\t24\tRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n11.0s\t25\tRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\n11.0s\t26\tRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.0.0)\n11.0s\t27\tRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\n11.0s\t28\tRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\n11.0s\t29\tRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n11.1s\t30\tRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\n11.1s\t31\tDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n11.1s\t32\t\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/410.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/410.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n14.4s\t33\t\u001b[?25hInstalling collected packages: nlpaug\n14.7s\t34\tSuccessfully installed nlpaug-1.1.11\n16.0s\t35\tRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\n16.0s\t36\tRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\n16.0s\t37\tRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n16.0s\t38\tRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n16.0s\t39\tRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n16.0s\t40\tRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n16.0s\t41\tRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n16.0s\t42\tRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n16.0s\t43\tRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\n16.0s\t44\tRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n16.0s\t45\tRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n16.0s\t46\tRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.3.8)\n16.0s\t47\tRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.2.4)\n16.0s\t48\tRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (0.1.1)\n16.0s\t49\tRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2025.0.1)\n16.0s\t50\tRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2022.0.0)\n16.0s\t51\tRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2.4.1)\n16.0s\t52\tRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n16.0s\t53\tRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2024.2.0)\n16.0s\t54\tRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2022.0.0)\n16.0s\t55\tRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\n16.0s\t56\tRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\n16.0s\t57\tRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optuna) (2024.2.0)\n34.1s\t58\t2025-01-24 08:46:58.819626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n34.3s\t59\t2025-01-24 08:46:59.075624: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n34.4s\t60\t2025-01-24 08:46:59.145258: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n90.0s\t61\tUsing device: cuda\n115.3s\t62\t[I 2025-01-24 08:48:20,029] A new study created in memory with name: no-name-681e1e96-a752-456d-99a5-298689dcb266\n117.1s\t63\t<ipython-input-8-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n117.1s\t64\t  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n1162.8s\t65\t[I 2025-01-24 09:05:47,567] Trial 0 finished with value: 0.6890894175553732 and parameters: {'learning_rate': 1.4818151091980784e-05, 'num_trainable_layers': 4, 'dropout_rate': 0.119110690825033, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8960630325737292, 'epochs': 3}. Best is trial 0 with value: 0.6890894175553732.\n3083.8s\t66\t[I 2025-01-24 09:37:48,516] Trial 1 finished with value: 0.8728465955701394 and parameters: {'learning_rate': 2.517549631081625e-05, 'num_trainable_layers': 6, 'dropout_rate': 0.41403406363825646, 'batch_size': 16, 'step_size': 4, 'gamma': 0.42358721860354653, 'epochs': 5}. Best is trial 1 with value: 0.8728465955701394.\n4771.2s\t67\t[I 2025-01-24 10:05:55,931] Trial 2 finished with value: 0.8884331419196062 and parameters: {'learning_rate': 5.202379803067906e-05, 'num_trainable_layers': 3, 'dropout_rate': 0.298865675518514, 'batch_size': 32, 'step_size': 5, 'gamma': 0.7444522192227857, 'epochs': 5}. Best is trial 2 with value: 0.8884331419196062.\n5828.5s\t68\t[I 2025-01-24 10:23:33,239] Trial 3 finished with value: 0.7875307629204266 and parameters: {'learning_rate': 0.00016564874914610164, 'num_trainable_layers': 4, 'dropout_rate': 0.24484638353248456, 'batch_size': 32, 'step_size': 8, 'gamma': 0.7133676325508109, 'epochs': 3}. Best is trial 2 with value: 0.8884331419196062.\n7368.5s\t69\t[I 2025-01-24 10:49:13,274] Trial 4 finished with value: 0.8310090237899918 and parameters: {'learning_rate': 6.24880645646062e-05, 'num_trainable_layers': 6, 'dropout_rate': 0.1787526406980196, 'batch_size': 16, 'step_size': 9, 'gamma': 0.4218077557604636, 'epochs': 4}. Best is trial 2 with value: 0.8884331419196062.\n9207.9s\t70\t[I 2025-01-24 11:19:52,632] Trial 5 finished with value: 0.8835110746513536 and parameters: {'learning_rate': 5.770893676901601e-05, 'num_trainable_layers': 5, 'dropout_rate': 0.13033142617846433, 'batch_size': 32, 'step_size': 6, 'gamma': 0.41133703177026737, 'epochs': 5}. Best is trial 2 with value: 0.8884331419196062.\n11037.0s\t71\t[I 2025-01-24 11:50:21,737] Trial 6 finished with value: 0.874487284659557 and parameters: {'learning_rate': 0.00013303199203240556, 'num_trainable_layers': 5, 'dropout_rate': 0.34849991659568424, 'batch_size': 64, 'step_size': 2, 'gamma': 0.41341582314867265, 'epochs': 5}. Best is trial 2 with value: 0.8884331419196062.\n11763.7s\t72\t[I 2025-01-24 12:02:28,472] Trial 7 finished with value: 0.11566858080393766 and parameters: {'learning_rate': 0.0005831206497447505, 'num_trainable_layers': 5, 'dropout_rate': 0.13925389521423429, 'batch_size': 32, 'step_size': 5, 'gamma': 0.5817810547623458, 'epochs': 2}. Best is trial 2 with value: 0.8884331419196062.\n12707.8s\t73\t[I 2025-01-24 12:18:12,553] Trial 8 finished with value: 0.26579163248564397 and parameters: {'learning_rate': 0.0008846062157467111, 'num_trainable_layers': 2, 'dropout_rate': 0.10968685543143071, 'batch_size': 32, 'step_size': 4, 'gamma': 0.5417040989520252, 'epochs': 3}. Best is trial 2 with value: 0.8884331419196062.\n14241.5s\t74\t[I 2025-01-24 12:43:46,253] Trial 9 finished with value: 0.8884331419196062 and parameters: {'learning_rate': 0.00019879063323214105, 'num_trainable_layers': 1, 'dropout_rate': 0.4669940144299951, 'batch_size': 32, 'step_size': 8, 'gamma': 0.5409999740245737, 'epochs': 5}. Best is trial 2 with value: 0.8884331419196062.\n15542.1s\t75\t[I 2025-01-24 13:05:26,876] Trial 10 finished with value: 0.5176374077112387 and parameters: {'learning_rate': 2.9798315739368094e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.29656423235918294, 'batch_size': 64, 'step_size': 2, 'gamma': 0.1813949711449518, 'epochs': 4}. Best is trial 2 with value: 0.8884331419196062.\n17074.1s\t76\t[I 2025-01-24 13:30:58,862] Trial 11 finished with value: 0.8851517637407711 and parameters: {'learning_rate': 0.0003659511106558045, 'num_trainable_layers': 1, 'dropout_rate': 0.47905495553653893, 'batch_size': 32, 'step_size': 7, 'gamma': 0.7655793929360052, 'epochs': 5}. Best is trial 2 with value: 0.8884331419196062.\n18303.5s\t77\t[I 2025-01-24 13:51:28,268] Trial 12 finished with value: 0.8859721082854799 and parameters: {'learning_rate': 0.00024783443813555816, 'num_trainable_layers': 1, 'dropout_rate': 0.37504869040198946, 'batch_size': 32, 'step_size': 10, 'gamma': 0.6504820771728465, 'epochs': 4}. Best is trial 2 with value: 0.8884331419196062.\n19990.2s\t78\t[I 2025-01-24 14:19:34,927] Trial 13 finished with value: 0.8884331419196062 and parameters: {'learning_rate': 8.01191130718201e-05, 'num_trainable_layers': 3, 'dropout_rate': 0.4826522544681453, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8440203634438823, 'epochs': 5}. Best is trial 2 with value: 0.8884331419196062.\n21280.1s\t79\t[I 2025-01-24 14:41:04,810] Trial 14 finished with value: 0.8334700574241182 and parameters: {'learning_rate': 3.9201057232782166e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.2609609742681635, 'batch_size': 32, 'step_size': 8, 'gamma': 0.6413069876133749, 'epochs': 4}. Best is trial 2 with value: 0.8884331419196062.\n22973.8s\t80\t[I 2025-01-24 15:09:18,497] Trial 15 finished with value: 0.8900738310090238 and parameters: {'learning_rate': 0.00021978865018528252, 'num_trainable_layers': 3, 'dropout_rate': 0.4103056673589882, 'batch_size': 64, 'step_size': 6, 'gamma': 0.2524299563975234, 'epochs': 5}. Best is trial 15 with value: 0.8900738310090238.\n23653.2s\t81\t[I 2025-01-24 15:20:37,923] Trial 16 finished with value: 0.31009023789991796 and parameters: {'learning_rate': 1.2999872686451634e-05, 'num_trainable_layers': 3, 'dropout_rate': 0.346836119246987, 'batch_size': 64, 'step_size': 5, 'gamma': 0.15456539728381552, 'epochs': 2}. Best is trial 15 with value: 0.8900738310090238.\n25005.7s\t82\t[I 2025-01-24 15:43:10,446] Trial 17 finished with value: 0.734208367514356 and parameters: {'learning_rate': 0.00033573893216988217, 'num_trainable_layers': 3, 'dropout_rate': 0.4157701006442217, 'batch_size': 64, 'step_size': 1, 'gamma': 0.2733314481355774, 'epochs': 4}. Best is trial 15 with value: 0.8900738310090238.\n26767.6s\t83\t[I 2025-01-24 16:12:32,354] Trial 18 finished with value: 0.8884331419196062 and parameters: {'learning_rate': 0.00010764068509778241, 'num_trainable_layers': 4, 'dropout_rate': 0.22330447689964178, 'batch_size': 64, 'step_size': 6, 'gamma': 0.29735957431573334, 'epochs': 5}. Best is trial 15 with value: 0.8900738310090238.\n28121.0s\t84\t[I 2025-01-24 16:35:05,702] Trial 19 finished with value: 0.77850697292863 and parameters: {'learning_rate': 4.441761716834638e-05, 'num_trainable_layers': 3, 'dropout_rate': 0.30322960836332824, 'batch_size': 64, 'step_size': 3, 'gamma': 0.28389315769469425, 'epochs': 4}. Best is trial 15 with value: 0.8900738310090238.\n29743.6s\t85\t[I 2025-01-24 17:02:08,289] Trial 20 finished with value: 0.6694011484823625 and parameters: {'learning_rate': 2.0683734117834178e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.4221268085035388, 'batch_size': 64, 'step_size': 7, 'gamma': 0.10765187998547693, 'epochs': 5}. Best is trial 15 with value: 0.8900738310090238.\n31277.1s\t86\t[I 2025-01-24 17:27:41,863] Trial 21 finished with value: 0.8999179655455292 and parameters: {'learning_rate': 0.0001932713713098696, 'num_trainable_layers': 1, 'dropout_rate': 0.4517023694187439, 'batch_size': 32, 'step_size': 8, 'gamma': 0.7710877288358753, 'epochs': 5}. Best is trial 21 with value: 0.8999179655455292.\n32888.7s\t87\t[I 2025-01-24 17:54:33,421] Trial 22 finished with value: 0.8982772764561116 and parameters: {'learning_rate': 0.00010003900869814402, 'num_trainable_layers': 2, 'dropout_rate': 0.43953240939272936, 'batch_size': 32, 'step_size': 10, 'gamma': 0.790299268957037, 'epochs': 5}. Best is trial 21 with value: 0.8999179655455292.\n34403.4s\t88\t[I 2025-01-24 18:19:48,093] Trial 23 finished with value: 0.8679245283018868 and parameters: {'learning_rate': 0.00027631868435139663, 'num_trainable_layers': 1, 'dropout_rate': 0.44978345529451597, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8175774891200873, 'epochs': 5}. Best is trial 21 with value: 0.8999179655455292.\n35696.0s\t89\t[I 2025-01-24 18:41:20,712] Trial 24 finished with value: 0.8720262510254306 and parameters: {'learning_rate': 0.00013348467042746236, 'num_trainable_layers': 2, 'dropout_rate': 0.4998480936674242, 'batch_size': 32, 'step_size': 9, 'gamma': 0.67408644365238, 'epochs': 4}. Best is trial 21 with value: 0.8999179655455292.\n37319.2s\t90\t[I 2025-01-24 19:08:23,900] Trial 25 finished with value: 0.8728465955701394 and parameters: {'learning_rate': 0.00046130960721521693, 'num_trainable_layers': 2, 'dropout_rate': 0.37594013765889545, 'batch_size': 64, 'step_size': 9, 'gamma': 0.813857333625018, 'epochs': 5}. Best is trial 21 with value: 0.8999179655455292.\n38547.0s\t91\t[I 2025-01-24 19:28:51,714] Trial 26 finished with value: 0.8851517637407711 and parameters: {'learning_rate': 0.0001564700242114712, 'num_trainable_layers': 1, 'dropout_rate': 0.4407740355882178, 'batch_size': 32, 'step_size': 10, 'gamma': 0.3389991488731838, 'epochs': 4}. Best is trial 21 with value: 0.8999179655455292.\n40155.7s\t92\t[I 2025-01-24 19:55:40,455] Trial 27 finished with value: 0.904019688269073 and parameters: {'learning_rate': 8.300432875328772e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.3847406475130443, 'batch_size': 32, 'step_size': 9, 'gamma': 0.5951936405857416, 'epochs': 5}. Best is trial 27 with value: 0.904019688269073.\n41077.2s\t93\t[I 2025-01-24 20:11:01,929] Trial 28 finished with value: 0.8400328137817884 and parameters: {'learning_rate': 8.067582306165917e-05, 'num_trainable_layers': 1, 'dropout_rate': 0.3811454948605194, 'batch_size': 32, 'step_size': 9, 'gamma': 0.8948541994871584, 'epochs': 3}. Best is trial 27 with value: 0.904019688269073.\n42037.1s\t94\t[I 2025-01-24 20:27:01,824] Trial 29 finished with value: 0.8630024610336341 and parameters: {'learning_rate': 9.029252417928428e-05, 'num_trainable_layers': 2, 'dropout_rate': 0.33361246073388606, 'batch_size': 16, 'step_size': 10, 'gamma': 0.5930182416596956, 'epochs': 3}. Best is trial 27 with value: 0.904019688269073.","metadata":{}}]}
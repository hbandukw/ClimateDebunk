{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install codecarbon\n",
        "# !pip install optuna"
      ],
      "metadata": {
        "id": "z1FKp173NXpc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\n",
        "from sklearn.metrics import f1_score, confusion_matrix, balanced_accuracy_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.optim import AdamW, lr_scheduler\n",
        "import shutil\n",
        "import zipfile\n",
        "# import optuna\n",
        "import copy\n",
        "from torch.nn.utils import prune\n",
        "import io\n",
        "# from codecarbon import EmissionsTracker\n",
        "\n",
        "# Check if GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A7JozZO9svy",
        "outputId": "c02a45a4-3e63-46e2-ef02-84861c5e535c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n",
        "    model = DistilBertForSequenceClassification(config)\n",
        "    state_dict = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Failed to load the model: {e}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Model file not found at {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "zNRJNcsLMwgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/distilbert_trained.pth'\n",
        "config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n",
        "model = DistilBertForSequenceClassification(config)\n",
        "\n",
        "# Load the pre-trained model weights\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Assuming 'device' is defined (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP-nMYB_9wE3",
        "outputId": "2919dfe1-adda-4b2e-cbb9-f1f6139ac638"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-b15e22a86284>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, val_loader, device):\n",
        "    # Ensure only one tracker instance runs at a time\n",
        "    tracker = EmissionsTracker(allow_multiple_runs=True)\n",
        "    tracker.start()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "                val_loss += outputs.loss.item()\n",
        "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_true_labels.extend(batch['labels'].cpu().numpy())\n",
        "                correct_val += (predictions == batch['labels']).sum().item()\n",
        "                total_val += batch['labels'].size(0)\n",
        "        average_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = correct_val / total_val\n",
        "\n",
        "    finally:\n",
        "        emissions = tracker.stop()\n",
        "        if emissions:\n",
        "            total_energy_used = getattr(emissions, 'energy_consumed', 0)\n",
        "        else:\n",
        "            total_energy_used = 0  # Default to 0 if emissions data is not available\n",
        "\n",
        "    return average_val_loss, accuracy, all_predictions, all_true_labels, total_energy_used\n"
      ],
      "metadata": {
        "id": "NBnCizGaRA7i"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"/content/test-00000-of-00001.parquet\")\n",
        "df['label_int'] = df['label'].str.split(\"_\").str[0].astype('int')\n",
        "\n",
        "texts = df[\"quote\"].to_list()\n",
        "labels = df[\"label_int\"].to_list()"
      ],
      "metadata": {
        "id": "hNKGjWQB968M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = len(df)\n",
        "print(num_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nL3lBfU2NF8",
        "outputId": "477933a2-db98-4418-84fa-91bfb8206658"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
        "MAX_LENGTH = 365\n",
        "\n",
        "# Dataset and DataLoader preparation\n",
        "class QuotesDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def encode_data(tokenizer, texts, labels, max_length):\n",
        "    try:\n",
        "        if isinstance(texts, pd.Series):\n",
        "            texts = texts.tolist()\n",
        "        if isinstance(labels, pd.Series):\n",
        "            labels = labels.tolist()\n",
        "\n",
        "        encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
        "        return QuotesDataset(encodings, labels)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during tokenization: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "gwwoYvXp9-Fm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = encode_data(tokenizer, texts, labels, MAX_LENGTH)\n",
        "val_loader = DataLoader(val_dataset, batch_size= 16, shuffle=False)"
      ],
      "metadata": {
        "id": "ORSD38hw-KFu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy, all_predictions, all_true_labels, emissions = validate_model(model, val_loader, device)\n",
        "print(val_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK50EiRp-KjR",
        "outputId": "973761bf-2fa9-4d7c-f2a2-4683cdfd1981"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 22:54:12] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:54:12] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:54:12] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:54:13] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:54:13] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:54:13] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:54:13] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:54:13] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:54:13]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:54:13]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:54:13]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:54:13]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:54:13]   CPU count: 2\n",
            "[codecarbon INFO @ 22:54:13]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:54:13]   GPU count: 1\n",
            "[codecarbon INFO @ 22:54:13]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:54:13] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:54:26] Energy consumed for RAM : 0.000017 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:54:26] Energy consumed for all CPUs : 0.000153 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:54:26] Energy consumed for all GPUs : 0.000250 kWh. Total GPU Power : 69.57287503481488 W\n",
            "[codecarbon INFO @ 22:54:26] 0.000420 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9794913863822805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def prune_model(model, amount):\n",
        "#     for name, module in model.named_modules():\n",
        "#         if isinstance(module, torch.nn.Linear):  # Prune linear layers as an example\n",
        "#             prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "\n",
        "# # Assuming 'model' is your pre-trained model and 'val_loader' is set up\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# pruning_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "# results = {}\n",
        "\n",
        "# for level in pruning_levels:\n",
        "#     prune_model(model, amount=level)\n",
        "#     val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n",
        "#     print(f\"Pruning at {level*100}% leads to an accuracy of {val_accuracy:.2f}%\")\n",
        "#     # Optionally reset the model to its original state if needed\n"
      ],
      "metadata": {
        "id": "0dE3fLRIBdZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pruning(model, pruning_params):\n",
        "    # Iterate over all named parameters in the model\n",
        "    for name, module in model.named_modules():\n",
        "        # Apply pruning to specific layers based on the layer's name and the predefined pruning parameters\n",
        "        if name in pruning_params:\n",
        "            # Check if the layer is a Linear layer and has a 'weight' attribute to prune\n",
        "            if hasattr(module, 'weight'):\n",
        "                prune.l1_unstructured(module, name='weight', amount=pruning_params[name])\n",
        "\n",
        "def objective(trial):\n",
        "    # Create a dictionary where each entry corresponds to a specific layer and its suggested pruning rate\n",
        "    pruning_params = {\n",
        "        'distilbert.transformer.layer.0.ffn.lin1': trial.suggest_float(\"lin1_layer0_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.0.ffn.lin2': trial.suggest_float(\"lin2_layer0_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.1.ffn.lin1': trial.suggest_float(\"lin1_layer1_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.1.ffn.lin2': trial.suggest_float(\"lin2_layer1_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.2.ffn.lin1': trial.suggest_float(\"lin1_layer2_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.2.ffn.lin2': trial.suggest_float(\"lin2_layer2_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.3.ffn.lin1': trial.suggest_float(\"lin1_layer3_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.3.ffn.lin2': trial.suggest_float(\"lin2_layer3_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.4.ffn.lin1': trial.suggest_float(\"lin1_layer4_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.4.ffn.lin2': trial.suggest_float(\"lin2_layer4_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.5.ffn.lin1': trial.suggest_float(\"lin1_layer5_pruning\", 0.1, 0.5),\n",
        "        'distilbert.transformer.layer.5.ffn.lin2': trial.suggest_float(\"lin2_layer5_pruning\", 0.1, 0.5)\n",
        "    }\n",
        "\n",
        "    # Load your model\n",
        "    config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n",
        "    model = DistilBertForSequenceClassification(config)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "\n",
        "    # Apply the pruning as per the trial's suggestion\n",
        "    apply_pruning(model, pruning_params)\n",
        "\n",
        "    # Validate the pruned model\n",
        "    val_loss, val_accuracy, all_predictions, all_true_labels, emissions = validate_model(model, val_loader, device)\n",
        "    trial.set_user_attr(\"emissions\", emissions)\n",
        "    trial.set_user_attr(\"accuracy\", val_accuracy)\n",
        "\n",
        "\n",
        "    return val_accuracy"
      ],
      "metadata": {
        "id": "RpdMGwuTEzfy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CafkHXl6FMXx",
        "outputId": "400d9dc7-0435-4653-ad0a-20c8a6d4e984"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-01-29 22:54:38,737] A new study created in memory with name: no-name-403e6edf-7f17-4b56-9528-f091b518bde2\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:54:40] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:54:40] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:54:40] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:54:40] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:54:41] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:54:41] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:54:41] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:54:41] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:54:41] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:54:41]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:54:41]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:54:41]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:54:41]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:54:41]   CPU count: 2\n",
            "[codecarbon INFO @ 22:54:41]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:54:41]   GPU count: 1\n",
            "[codecarbon INFO @ 22:54:41]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:54:41] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:54:41] Energy consumed for RAM : 0.000416 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:54:41] Energy consumed for all CPUs : 0.003718 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:54:41] Energy consumed for all GPUs : 0.003153 kWh. Total GPU Power : 35.207725400719156 W\n",
            "[codecarbon INFO @ 22:54:41] 0.007287 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:54:54] Energy consumed for RAM : 0.000017 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:54:54] Energy consumed for all CPUs : 0.000148 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:54:54] Energy consumed for all GPUs : 0.000242 kWh. Total GPU Power : 69.59548098933695 W\n",
            "[codecarbon INFO @ 22:54:54] 0.000407 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:54:54,411] Trial 0 finished with value: 0.9696472518457753 and parameters: {'lin1_layer0_pruning': 0.34902498682350014, 'lin2_layer0_pruning': 0.2663574573779657, 'lin1_layer1_pruning': 0.29071701976783426, 'lin2_layer1_pruning': 0.22770338719052008, 'lin1_layer2_pruning': 0.41563454190185545, 'lin2_layer2_pruning': 0.1904375184738131, 'lin1_layer3_pruning': 0.24606974793087122, 'lin2_layer3_pruning': 0.11199921071649649, 'lin1_layer4_pruning': 0.3270273658967524, 'lin2_layer4_pruning': 0.49062701342545445, 'lin1_layer5_pruning': 0.29704284082295696, 'lin2_layer5_pruning': 0.36842745183165815}. Best is trial 0 with value: 0.9696472518457753.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:54:55] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:54:55] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:54:55] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:54:55] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 22:54:56] Energy consumed for RAM : 0.000436 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:54:56] Energy consumed for all CPUs : 0.003895 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:54:57] Energy consumed for all GPUs : 0.003422 kWh. Total GPU Power : 64.51124442554645 W\n",
            "[codecarbon INFO @ 22:54:57] 0.007752 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 22:54:57] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:54:57] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:54:57] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:54:57] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:54:57] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:54:57]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:54:57]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:54:57]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:54:57]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:54:57]   CPU count: 2\n",
            "[codecarbon INFO @ 22:54:57]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:54:57]   GPU count: 1\n",
            "[codecarbon INFO @ 22:54:57]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:54:57] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:55:09] Energy consumed for RAM : 0.000017 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:09] Energy consumed for all CPUs : 0.000149 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:09] Energy consumed for all GPUs : 0.000243 kWh. Total GPU Power : 69.33227832976739 W\n",
            "[codecarbon INFO @ 22:55:09] 0.000409 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:55:09,843] Trial 1 finished with value: 0.9647251845775225 and parameters: {'lin1_layer0_pruning': 0.2036157940018605, 'lin2_layer0_pruning': 0.32815261178198496, 'lin1_layer1_pruning': 0.23457674669195763, 'lin2_layer1_pruning': 0.2669465040269411, 'lin1_layer2_pruning': 0.4842014396140166, 'lin2_layer2_pruning': 0.4540600136222198, 'lin1_layer3_pruning': 0.4429454221410163, 'lin2_layer3_pruning': 0.4180249511214533, 'lin1_layer4_pruning': 0.24001721045454982, 'lin2_layer4_pruning': 0.2770257435651572, 'lin1_layer5_pruning': 0.16553929626188568, 'lin2_layer5_pruning': 0.2030906694880558}. Best is trial 0 with value: 0.9696472518457753.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:55:11] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:55:11] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:55:11] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:55:11] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 22:55:12] Energy consumed for RAM : 0.000455 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:12] Energy consumed for all CPUs : 0.004072 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:12] Energy consumed for all GPUs : 0.003695 kWh. Total GPU Power : 65.37767500095802 W\n",
            "[codecarbon INFO @ 22:55:12] 0.008222 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 22:55:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:55:12] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:12] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:55:12] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:55:12] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:55:12]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:55:12]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:55:12]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:55:12]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:55:12]   CPU count: 2\n",
            "[codecarbon INFO @ 22:55:12]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:12]   GPU count: 1\n",
            "[codecarbon INFO @ 22:55:12]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:55:12] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:55:25] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:25] Energy consumed for all CPUs : 0.000146 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:25] Energy consumed for all GPUs : 0.000239 kWh. Total GPU Power : 69.70819768625415 W\n",
            "[codecarbon INFO @ 22:55:25] 0.000401 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:55:25,039] Trial 2 finished with value: 0.9696472518457753 and parameters: {'lin1_layer0_pruning': 0.16186121798900877, 'lin2_layer0_pruning': 0.46677413491799125, 'lin1_layer1_pruning': 0.2166294870949076, 'lin2_layer1_pruning': 0.32202373839358656, 'lin1_layer2_pruning': 0.48339085483107447, 'lin2_layer2_pruning': 0.20680162623832046, 'lin1_layer3_pruning': 0.43030023151366015, 'lin2_layer3_pruning': 0.45735080200045386, 'lin1_layer4_pruning': 0.31103853025703065, 'lin2_layer4_pruning': 0.10666544893238719, 'lin1_layer5_pruning': 0.38974347722549163, 'lin2_layer5_pruning': 0.4823643947196108}. Best is trial 0 with value: 0.9696472518457753.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:55:26] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:55:26] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:55:26] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:55:26] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 22:55:27] Energy consumed for RAM : 0.000475 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:27] Energy consumed for all CPUs : 0.004249 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:27] Energy consumed for all GPUs : 0.003965 kWh. Total GPU Power : 64.89452748967969 W\n",
            "[codecarbon INFO @ 22:55:27] 0.008689 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:55:27] 0.009372 g.CO2eq/s mean an estimation of 295.5591872072073 kg.CO2eq/year\n",
            "[codecarbon WARNING @ 22:55:27] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:55:27] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:27] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:55:27] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:55:27] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:55:27]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:55:27]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:55:27]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:55:27]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:55:27]   CPU count: 2\n",
            "[codecarbon INFO @ 22:55:27]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:27]   GPU count: 1\n",
            "[codecarbon INFO @ 22:55:27]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:55:27] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:55:39] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:39] Energy consumed for all CPUs : 0.000144 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:40] Energy consumed for all GPUs : 0.000235 kWh. Total GPU Power : 69.38007811172004 W\n",
            "[codecarbon INFO @ 22:55:40] 0.000395 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:55:40,019] Trial 3 finished with value: 0.9524200164068909 and parameters: {'lin1_layer0_pruning': 0.26112651889650534, 'lin2_layer0_pruning': 0.3692042069152901, 'lin1_layer1_pruning': 0.46042481227385346, 'lin2_layer1_pruning': 0.3452751949596071, 'lin1_layer2_pruning': 0.4971359190020067, 'lin2_layer2_pruning': 0.492719227767899, 'lin1_layer3_pruning': 0.19719939211343132, 'lin2_layer3_pruning': 0.24943913180243707, 'lin1_layer4_pruning': 0.22915140981547683, 'lin2_layer4_pruning': 0.45112057401056105, 'lin1_layer5_pruning': 0.15682509020452162, 'lin2_layer5_pruning': 0.4095757505161949}. Best is trial 0 with value: 0.9696472518457753.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon INFO @ 22:55:42] Energy consumed for RAM : 0.000495 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:42] Energy consumed for all CPUs : 0.004426 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:42] Energy consumed for all GPUs : 0.004231 kWh. Total GPU Power : 63.902679223224204 W\n",
            "[codecarbon INFO @ 22:55:42] 0.009153 kWh of electricity used since the beginning.\n",
            "[codecarbon WARNING @ 22:55:42] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:55:42] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:55:42] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:55:42] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:55:43] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:55:43] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:43] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:55:43] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:55:43] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:55:43]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:55:43]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:55:43]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:55:43]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:55:43]   CPU count: 2\n",
            "[codecarbon INFO @ 22:55:43]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:43]   GPU count: 1\n",
            "[codecarbon INFO @ 22:55:43]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:55:43] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:55:56] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:56] Energy consumed for all CPUs : 0.000145 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:56] Energy consumed for all GPUs : 0.000236 kWh. Total GPU Power : 69.49780234094906 W\n",
            "[codecarbon INFO @ 22:55:56] 0.000397 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:55:56,045] Trial 4 finished with value: 0.9729286300246104 and parameters: {'lin1_layer0_pruning': 0.14828764597238459, 'lin2_layer0_pruning': 0.28944435859186257, 'lin1_layer1_pruning': 0.353310129750813, 'lin2_layer1_pruning': 0.15624439486547834, 'lin1_layer2_pruning': 0.27781010828950714, 'lin2_layer2_pruning': 0.3636696768111284, 'lin1_layer3_pruning': 0.45392970348941575, 'lin2_layer3_pruning': 0.2441098225527113, 'lin1_layer4_pruning': 0.34580465587113707, 'lin2_layer4_pruning': 0.2902273336404029, 'lin1_layer5_pruning': 0.11010900298856306, 'lin2_layer5_pruning': 0.2203275453039809}. Best is trial 4 with value: 0.9729286300246104.\n",
            "[codecarbon INFO @ 22:55:57] Energy consumed for RAM : 0.000515 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:55:57] Energy consumed for all CPUs : 0.004603 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:55:57] Energy consumed for all GPUs : 0.004497 kWh. Total GPU Power : 63.91135574072129 W\n",
            "[codecarbon INFO @ 22:55:57] 0.009615 kWh of electricity used since the beginning.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:55:57] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:55:57] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:55:57] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:55:57] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:55:58] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:55:58] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:58] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:55:58] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:55:58] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:55:58]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:55:58]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:55:58]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:55:58]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:55:58]   CPU count: 2\n",
            "[codecarbon INFO @ 22:55:58]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:55:58]   GPU count: 1\n",
            "[codecarbon INFO @ 22:55:58]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:55:58] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:56:11] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:11] Energy consumed for all CPUs : 0.000146 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:11] Energy consumed for all GPUs : 0.000238 kWh. Total GPU Power : 69.58650505968372 W\n",
            "[codecarbon INFO @ 22:56:11] 0.000400 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:56:11,295] Trial 5 finished with value: 0.963084495488105 and parameters: {'lin1_layer0_pruning': 0.1256232805433069, 'lin2_layer0_pruning': 0.40294564095838314, 'lin1_layer1_pruning': 0.46087093981347393, 'lin2_layer1_pruning': 0.31950834424476604, 'lin1_layer2_pruning': 0.2679252571863105, 'lin2_layer2_pruning': 0.48052022462805755, 'lin1_layer3_pruning': 0.34275400737714923, 'lin2_layer3_pruning': 0.3895630096191657, 'lin1_layer4_pruning': 0.1729998828885167, 'lin2_layer4_pruning': 0.17459353952497014, 'lin1_layer5_pruning': 0.48197284545697894, 'lin2_layer5_pruning': 0.3739807557192164}. Best is trial 4 with value: 0.9729286300246104.\n",
            "[codecarbon INFO @ 22:56:12] Energy consumed for RAM : 0.000534 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:12] Energy consumed for all CPUs : 0.004780 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:12] Energy consumed for all GPUs : 0.004767 kWh. Total GPU Power : 64.70561084721518 W\n",
            "[codecarbon INFO @ 22:56:12] 0.010081 kWh of electricity used since the beginning.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:56:12] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:56:12] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:56:12] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:56:12] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:56:13] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:56:13] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:56:13] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:56:13] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:56:14] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:56:14]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:56:14]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:56:14]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:56:14]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:56:14]   CPU count: 2\n",
            "[codecarbon INFO @ 22:56:14]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:56:14]   GPU count: 1\n",
            "[codecarbon INFO @ 22:56:14]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:56:14] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:56:26] Energy consumed for RAM : 0.000017 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:26] Energy consumed for all CPUs : 0.000148 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:26] Energy consumed for all GPUs : 0.000241 kWh. Total GPU Power : 69.28875004277853 W\n",
            "[codecarbon INFO @ 22:56:26] 0.000405 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:56:26,656] Trial 6 finished with value: 0.9762100082034455 and parameters: {'lin1_layer0_pruning': 0.15616986206180836, 'lin2_layer0_pruning': 0.4837853268735216, 'lin1_layer1_pruning': 0.35389426723515904, 'lin2_layer1_pruning': 0.19207744275555672, 'lin1_layer2_pruning': 0.3797945594746801, 'lin2_layer2_pruning': 0.2313535477498607, 'lin1_layer3_pruning': 0.27431769595956534, 'lin2_layer3_pruning': 0.1430018811523521, 'lin1_layer4_pruning': 0.2182130664853118, 'lin2_layer4_pruning': 0.2930671920261646, 'lin1_layer5_pruning': 0.22217022876990744, 'lin2_layer5_pruning': 0.31691412167666666}. Best is trial 6 with value: 0.9762100082034455.\n",
            "[codecarbon INFO @ 22:56:27] Energy consumed for RAM : 0.000554 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:27] Energy consumed for all CPUs : 0.004957 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:27] Energy consumed for all GPUs : 0.005036 kWh. Total GPU Power : 64.72716871471404 W\n",
            "[codecarbon INFO @ 22:56:27] 0.010548 kWh of electricity used since the beginning.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:56:28] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:56:28] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:56:28] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:56:28] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:56:29] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:56:29] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:56:29] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:56:29] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:56:29] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:56:29]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:56:29]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:56:29]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:56:29]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:56:29]   CPU count: 2\n",
            "[codecarbon INFO @ 22:56:29]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:56:29]   GPU count: 1\n",
            "[codecarbon INFO @ 22:56:29]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:56:29] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:56:41] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:41] Energy consumed for all CPUs : 0.000146 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:41] Energy consumed for all GPUs : 0.000240 kWh. Total GPU Power : 69.72693570532111 W\n",
            "[codecarbon INFO @ 22:56:41] 0.000403 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:56:41,877] Trial 7 finished with value: 0.9770303527481542 and parameters: {'lin1_layer0_pruning': 0.17938720019320123, 'lin2_layer0_pruning': 0.14935347327482285, 'lin1_layer1_pruning': 0.2183749123470145, 'lin2_layer1_pruning': 0.425480386200047, 'lin1_layer2_pruning': 0.1034831590727352, 'lin2_layer2_pruning': 0.17345266085189254, 'lin1_layer3_pruning': 0.24495757231063778, 'lin2_layer3_pruning': 0.35882204533983664, 'lin1_layer4_pruning': 0.3596960754970957, 'lin2_layer4_pruning': 0.13586461492970556, 'lin1_layer5_pruning': 0.4779885032960357, 'lin2_layer5_pruning': 0.14815929020314994}. Best is trial 7 with value: 0.9770303527481542.\n",
            "[codecarbon INFO @ 22:56:42] Energy consumed for RAM : 0.000574 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:42] Energy consumed for all CPUs : 0.005134 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:42] Energy consumed for all GPUs : 0.005304 kWh. Total GPU Power : 64.27908762879841 W\n",
            "[codecarbon INFO @ 22:56:42] 0.011012 kWh of electricity used since the beginning.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:56:43] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:56:43] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:56:43] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:56:43] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:56:44] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:56:44] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:56:44] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:56:44] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:56:44] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:56:44]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:56:44]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:56:44]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:56:44]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:56:44]   CPU count: 2\n",
            "[codecarbon INFO @ 22:56:44]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:56:44]   GPU count: 1\n",
            "[codecarbon INFO @ 22:56:44]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:56:44] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:56:56] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:56] Energy consumed for all CPUs : 0.000145 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:56] Energy consumed for all GPUs : 0.000237 kWh. Total GPU Power : 69.55270771874969 W\n",
            "[codecarbon INFO @ 22:56:56] 0.000397 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:56:56,859] Trial 8 finished with value: 0.9680065627563577 and parameters: {'lin1_layer0_pruning': 0.2999085735656092, 'lin2_layer0_pruning': 0.4810304091936456, 'lin1_layer1_pruning': 0.4914392481389598, 'lin2_layer1_pruning': 0.1225233565628054, 'lin1_layer2_pruning': 0.1796058038730665, 'lin2_layer2_pruning': 0.3261463716701638, 'lin1_layer3_pruning': 0.10650759303995111, 'lin2_layer3_pruning': 0.4696953736585632, 'lin1_layer4_pruning': 0.4862454498605666, 'lin2_layer4_pruning': 0.2292898023864408, 'lin1_layer5_pruning': 0.4875720590625433, 'lin2_layer5_pruning': 0.24205932708033018}. Best is trial 7 with value: 0.9770303527481542.\n",
            "[codecarbon INFO @ 22:56:57] Energy consumed for RAM : 0.000594 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:56:57] Energy consumed for all CPUs : 0.005311 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:56:57] Energy consumed for all GPUs : 0.005573 kWh. Total GPU Power : 64.5088956702957 W\n",
            "[codecarbon INFO @ 22:56:57] 0.011478 kWh of electricity used since the beginning.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:56:58] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:56:58] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:56:58] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:56:58] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:56:59] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:56:59] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:56:59] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:56:59] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:56:59] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:56:59]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:56:59]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:56:59]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:56:59]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:56:59]   CPU count: 2\n",
            "[codecarbon INFO @ 22:56:59]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:56:59]   GPU count: 1\n",
            "[codecarbon INFO @ 22:56:59]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:56:59] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:57:11] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:11] Energy consumed for all CPUs : 0.000145 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:11] Energy consumed for all GPUs : 0.000238 kWh. Total GPU Power : 69.77100290981019 W\n",
            "[codecarbon INFO @ 22:57:11] 0.000399 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:57:11,932] Trial 9 finished with value: 0.970467596390484 and parameters: {'lin1_layer0_pruning': 0.3815143296012975, 'lin2_layer0_pruning': 0.40141167051745097, 'lin1_layer1_pruning': 0.35002517352865736, 'lin2_layer1_pruning': 0.10917973648148066, 'lin1_layer2_pruning': 0.40265365091272587, 'lin2_layer2_pruning': 0.22688274155844948, 'lin1_layer3_pruning': 0.12097751112355981, 'lin2_layer3_pruning': 0.25962720650426496, 'lin1_layer4_pruning': 0.24378752607966386, 'lin2_layer4_pruning': 0.38101763677412814, 'lin1_layer5_pruning': 0.34833326495187567, 'lin2_layer5_pruning': 0.2798883723717437}. Best is trial 7 with value: 0.9770303527481542.\n",
            "[codecarbon INFO @ 22:57:12] Energy consumed for RAM : 0.000614 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:12] Energy consumed for all CPUs : 0.005489 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:12] Energy consumed for all GPUs : 0.005842 kWh. Total GPU Power : 64.38774001857968 W\n",
            "[codecarbon INFO @ 22:57:12] 0.011944 kWh of electricity used since the beginning.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:57:15] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:57:15] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:57:15] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:57:15] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:57:16] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:57:16] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:57:16] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:57:16] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:57:16] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:57:16]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:57:16]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:57:16]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:57:16]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:57:16]   CPU count: 2\n",
            "[codecarbon INFO @ 22:57:16]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:57:16]   GPU count: 1\n",
            "[codecarbon INFO @ 22:57:16]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:57:16] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:57:27] Energy consumed for RAM : 0.000634 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:27] Energy consumed for all CPUs : 0.005666 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:27] Energy consumed for all GPUs : 0.006083 kWh. Total GPU Power : 57.97119955944417 W\n",
            "[codecarbon INFO @ 22:57:27] 0.012382 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:57:27] 0.010741 g.CO2eq/s mean an estimation of 338.71561505363786 kg.CO2eq/year\n",
            "[codecarbon INFO @ 22:57:29] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:29] Energy consumed for all CPUs : 0.000145 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:29] Energy consumed for all GPUs : 0.000236 kWh. Total GPU Power : 69.33785403786486 W\n",
            "[codecarbon INFO @ 22:57:29] 0.000397 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:57:29,259] Trial 10 finished with value: 0.9671862182116489 and parameters: {'lin1_layer0_pruning': 0.48462456371348556, 'lin2_layer0_pruning': 0.11027422332262302, 'lin1_layer1_pruning': 0.14905433819400676, 'lin2_layer1_pruning': 0.47692432476333324, 'lin1_layer2_pruning': 0.10187263797095958, 'lin2_layer2_pruning': 0.1090266243067809, 'lin1_layer3_pruning': 0.36096963899337586, 'lin2_layer3_pruning': 0.33430966292160463, 'lin1_layer4_pruning': 0.4302125852504872, 'lin2_layer4_pruning': 0.10396330020182229, 'lin1_layer5_pruning': 0.4021976365126184, 'lin2_layer5_pruning': 0.11415258162143033}. Best is trial 7 with value: 0.9770303527481542.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:57:30] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:57:30] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:57:30] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:57:30] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:57:32] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:57:32] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:57:32] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:57:32] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:57:32] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:57:32]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:57:32]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:57:32]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:57:32]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:57:32]   CPU count: 2\n",
            "[codecarbon INFO @ 22:57:32]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:57:32]   GPU count: 1\n",
            "[codecarbon INFO @ 22:57:32]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:57:32] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:57:42] Energy consumed for RAM : 0.000653 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:42] Energy consumed for all CPUs : 0.005843 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:42] Energy consumed for all GPUs : 0.006346 kWh. Total GPU Power : 63.046105002596455 W\n",
            "[codecarbon INFO @ 22:57:42] 0.012842 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:57:44] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:44] Energy consumed for all CPUs : 0.000148 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:44] Energy consumed for all GPUs : 0.000240 kWh. Total GPU Power : 69.24326443715425 W\n",
            "[codecarbon INFO @ 22:57:44] 0.000405 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:57:44,807] Trial 11 finished with value: 0.9729286300246104 and parameters: {'lin1_layer0_pruning': 0.2062105910159028, 'lin2_layer0_pruning': 0.1780384051228706, 'lin1_layer1_pruning': 0.10428765481698041, 'lin2_layer1_pruning': 0.436066783684374, 'lin1_layer2_pruning': 0.35971428101119046, 'lin2_layer2_pruning': 0.11699531708278446, 'lin1_layer3_pruning': 0.260669879310713, 'lin2_layer3_pruning': 0.11682397810549726, 'lin1_layer4_pruning': 0.11143936785280736, 'lin2_layer4_pruning': 0.38102448355671625, 'lin1_layer5_pruning': 0.2485496874140894, 'lin2_layer5_pruning': 0.3326106947473068}. Best is trial 7 with value: 0.9770303527481542.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:57:46] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:57:46] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:57:46] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:57:46] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:57:47] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:57:47] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:57:47] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:57:47] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:57:47] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:57:47]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:57:47]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:57:47]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:57:47]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:57:47]   CPU count: 2\n",
            "[codecarbon INFO @ 22:57:47]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:57:47]   GPU count: 1\n",
            "[codecarbon INFO @ 22:57:47]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:57:47] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:57:57] Energy consumed for RAM : 0.000673 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:57:57] Energy consumed for all CPUs : 0.006020 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:57:57] Energy consumed for all GPUs : 0.006612 kWh. Total GPU Power : 63.770379295768464 W\n",
            "[codecarbon INFO @ 22:57:57] 0.013305 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:58:00] Energy consumed for RAM : 0.000017 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:00] Energy consumed for all CPUs : 0.000148 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:00] Energy consumed for all GPUs : 0.000243 kWh. Total GPU Power : 69.72623785759569 W\n",
            "[codecarbon INFO @ 22:58:00] 0.000407 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:58:00,301] Trial 12 finished with value: 0.9729286300246104 and parameters: {'lin1_layer0_pruning': 0.2357637896841839, 'lin2_layer0_pruning': 0.21498193396799525, 'lin1_layer1_pruning': 0.3603214823500875, 'lin2_layer1_pruning': 0.39179566855977566, 'lin1_layer2_pruning': 0.1895259805191988, 'lin2_layer2_pruning': 0.26387214091556754, 'lin1_layer3_pruning': 0.1963284905094392, 'lin2_layer3_pruning': 0.1915742485921846, 'lin1_layer4_pruning': 0.3910086566653099, 'lin2_layer4_pruning': 0.1843484377597589, 'lin1_layer5_pruning': 0.23363162463942921, 'lin2_layer5_pruning': 0.13806241560323926}. Best is trial 7 with value: 0.9770303527481542.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:58:02] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:58:02] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:58:02] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:58:02] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:58:03] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:58:03] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:58:03] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:58:03] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:58:03] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:58:03]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:58:03]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:58:03]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:58:03]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:58:03]   CPU count: 2\n",
            "[codecarbon INFO @ 22:58:03]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:58:03]   GPU count: 1\n",
            "[codecarbon INFO @ 22:58:03]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:58:03] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:58:12] Energy consumed for RAM : 0.000693 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:12] Energy consumed for all CPUs : 0.006197 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:12] Energy consumed for all GPUs : 0.006870 kWh. Total GPU Power : 62.10063326738279 W\n",
            "[codecarbon INFO @ 22:58:12] 0.013760 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:58:16] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:16] Energy consumed for all CPUs : 0.000146 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:16] Energy consumed for all GPUs : 0.000239 kWh. Total GPU Power : 69.32958642909148 W\n",
            "[codecarbon INFO @ 22:58:16] 0.000401 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:58:16,294] Trial 13 finished with value: 0.9721082854799016 and parameters: {'lin1_layer0_pruning': 0.10320047468232649, 'lin2_layer0_pruning': 0.10040983683144453, 'lin1_layer1_pruning': 0.26629164388507753, 'lin2_layer1_pruning': 0.21565701778182686, 'lin1_layer2_pruning': 0.3275653995542901, 'lin2_layer2_pruning': 0.16845579257415152, 'lin1_layer3_pruning': 0.320134340850713, 'lin2_layer3_pruning': 0.32676556870210083, 'lin1_layer4_pruning': 0.16551458069638691, 'lin2_layer4_pruning': 0.37037263028415945, 'lin1_layer5_pruning': 0.2371782084154906, 'lin2_layer5_pruning': 0.1569174733863769}. Best is trial 7 with value: 0.9770303527481542.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:58:17] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:58:17] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:58:17] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:58:17] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:58:18] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:58:18] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:58:18] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:58:18] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:58:19] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:58:19]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:58:19]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:58:19]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:58:19]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:58:19]   CPU count: 2\n",
            "[codecarbon INFO @ 22:58:19]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:58:19]   GPU count: 1\n",
            "[codecarbon INFO @ 22:58:19]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:58:19] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:58:27] Energy consumed for RAM : 0.000713 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:27] Energy consumed for all CPUs : 0.006374 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:27] Energy consumed for all GPUs : 0.007135 kWh. Total GPU Power : 63.63024483029348 W\n",
            "[codecarbon INFO @ 22:58:27] 0.014222 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:58:31] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:31] Energy consumed for all CPUs : 0.000146 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:31] Energy consumed for all GPUs : 0.000238 kWh. Total GPU Power : 69.38674717831924 W\n",
            "[codecarbon INFO @ 22:58:31] 0.000400 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:58:31,525] Trial 14 finished with value: 0.9770303527481542 and parameters: {'lin1_layer0_pruning': 0.17533409341472736, 'lin2_layer0_pruning': 0.1877086502993578, 'lin1_layer1_pruning': 0.1877410542172132, 'lin2_layer1_pruning': 0.4939461034226087, 'lin1_layer2_pruning': 0.10769155923590618, 'lin2_layer2_pruning': 0.27536613279153765, 'lin1_layer3_pruning': 0.2509603891662221, 'lin2_layer3_pruning': 0.1699594382769343, 'lin1_layer4_pruning': 0.3895600265401689, 'lin2_layer4_pruning': 0.2114580806075383, 'lin1_layer5_pruning': 0.3078607277442939, 'lin2_layer5_pruning': 0.2949021270291823}. Best is trial 7 with value: 0.9770303527481542.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:58:33] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:58:33] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:58:33] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:58:33] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:58:34] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:58:34] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:58:34] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:58:34] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:58:34] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:58:34]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:58:34]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:58:34]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:58:34]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:58:34]   CPU count: 2\n",
            "[codecarbon INFO @ 22:58:34]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:58:34]   GPU count: 1\n",
            "[codecarbon INFO @ 22:58:34]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:58:34] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:58:42] Energy consumed for RAM : 0.000732 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:42] Energy consumed for all CPUs : 0.006551 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:42] Energy consumed for all GPUs : 0.007398 kWh. Total GPU Power : 63.17065820031682 W\n",
            "[codecarbon INFO @ 22:58:42] 0.014681 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:58:47] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:47] Energy consumed for all CPUs : 0.000146 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:47] Energy consumed for all GPUs : 0.000238 kWh. Total GPU Power : 69.3472322605752 W\n",
            "[codecarbon INFO @ 22:58:47] 0.000400 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:58:47,190] Trial 15 finished with value: 0.9712879409351928 and parameters: {'lin1_layer0_pruning': 0.283284784379078, 'lin2_layer0_pruning': 0.18552840136024168, 'lin1_layer1_pruning': 0.17331406775949387, 'lin2_layer1_pruning': 0.49120000634860217, 'lin1_layer2_pruning': 0.11071245194282682, 'lin2_layer2_pruning': 0.2932855237754082, 'lin1_layer3_pruning': 0.18697474321150742, 'lin2_layer3_pruning': 0.3604188955348315, 'lin1_layer4_pruning': 0.40219315583871285, 'lin2_layer4_pruning': 0.19204439143755603, 'lin1_layer5_pruning': 0.42633491015996117, 'lin2_layer5_pruning': 0.17966332744681293}. Best is trial 7 with value: 0.9770303527481542.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:58:48] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:58:48] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:58:48] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:58:48] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:58:50] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:58:50] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:58:50] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:58:50] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:58:50] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:58:50]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:58:50]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:58:50]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:58:50]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:58:50]   CPU count: 2\n",
            "[codecarbon INFO @ 22:58:50]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:58:50]   GPU count: 1\n",
            "[codecarbon INFO @ 22:58:50]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:58:50] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:58:57] Energy consumed for RAM : 0.000752 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:58:57] Energy consumed for all CPUs : 0.006728 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:58:57] Energy consumed for all GPUs : 0.007662 kWh. Total GPU Power : 63.37617213333295 W\n",
            "[codecarbon INFO @ 22:58:57] 0.015142 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:59:02] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:02] Energy consumed for all CPUs : 0.000145 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:02] Energy consumed for all GPUs : 0.000239 kWh. Total GPU Power : 69.74031132507982 W\n",
            "[codecarbon INFO @ 22:59:02] 0.000400 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:59:02,509] Trial 16 finished with value: 0.9721082854799016 and parameters: {'lin1_layer0_pruning': 0.35497245863651655, 'lin2_layer0_pruning': 0.235618752290862, 'lin1_layer1_pruning': 0.19472077311313724, 'lin2_layer1_pruning': 0.41107063895684365, 'lin1_layer2_pruning': 0.17704969938112153, 'lin2_layer2_pruning': 0.40396377874199174, 'lin1_layer3_pruning': 0.38312020167612393, 'lin2_layer3_pruning': 0.18506128286893828, 'lin1_layer4_pruning': 0.37036546196408104, 'lin2_layer4_pruning': 0.145103627281786, 'lin1_layer5_pruning': 0.3301119355848241, 'lin2_layer5_pruning': 0.2540674118045519}. Best is trial 7 with value: 0.9770303527481542.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:59:03] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:59:03] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:59:03] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:59:03] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:59:05] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:59:05] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:59:05] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:59:05] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:59:05] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:59:05]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:59:05]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:59:05]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:59:05]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:59:05]   CPU count: 2\n",
            "[codecarbon INFO @ 22:59:05]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:59:05]   GPU count: 1\n",
            "[codecarbon INFO @ 22:59:05]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:59:05] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:59:12] Energy consumed for RAM : 0.000772 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:12] Energy consumed for all CPUs : 0.006905 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:12] Energy consumed for all GPUs : 0.007928 kWh. Total GPU Power : 63.720952199294636 W\n",
            "[codecarbon INFO @ 22:59:12] 0.015604 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:59:17] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:17] Energy consumed for all CPUs : 0.000146 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:17] Energy consumed for all GPUs : 0.000240 kWh. Total GPU Power : 69.88949300230011 W\n",
            "[codecarbon INFO @ 22:59:17] 0.000403 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:59:17,694] Trial 17 finished with value: 0.977850697292863 and parameters: {'lin1_layer0_pruning': 0.2017953262091607, 'lin2_layer0_pruning': 0.15729525440967862, 'lin1_layer1_pruning': 0.12969848850621088, 'lin2_layer1_pruning': 0.454859759610882, 'lin1_layer2_pruning': 0.2310579033256247, 'lin2_layer2_pruning': 0.1607487810536109, 'lin1_layer3_pruning': 0.22990726915583418, 'lin2_layer3_pruning': 0.29273042218157586, 'lin1_layer4_pruning': 0.44861219131635766, 'lin2_layer4_pruning': 0.23703770072386673, 'lin1_layer5_pruning': 0.45099619043007, 'lin2_layer5_pruning': 0.4751512722238028}. Best is trial 17 with value: 0.977850697292863.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:59:19] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:59:19] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:59:19] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:59:19] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:59:20] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:59:20] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:59:20] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:59:20] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:59:20] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:59:20]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:59:20]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:59:20]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:59:20]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:59:20]   CPU count: 2\n",
            "[codecarbon INFO @ 22:59:20]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:59:20]   GPU count: 1\n",
            "[codecarbon INFO @ 22:59:20]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:59:20] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:59:27] Energy consumed for RAM : 0.000792 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:27] Energy consumed for all CPUs : 0.007082 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:27] Energy consumed for all GPUs : 0.008194 kWh. Total GPU Power : 63.88292739245222 W\n",
            "[codecarbon INFO @ 22:59:27] 0.016067 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:59:27] 0.010721 g.CO2eq/s mean an estimation of 338.08344904582134 kg.CO2eq/year\n",
            "[codecarbon INFO @ 22:59:32] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:32] Energy consumed for all CPUs : 0.000147 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:32] Energy consumed for all GPUs : 0.000240 kWh. Total GPU Power : 69.63106651045055 W\n",
            "[codecarbon INFO @ 22:59:32] 0.000403 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:59:33,003] Trial 18 finished with value: 0.9762100082034455 and parameters: {'lin1_layer0_pruning': 0.22255334470954666, 'lin2_layer0_pruning': 0.12447180711850622, 'lin1_layer1_pruning': 0.10022398730997578, 'lin2_layer1_pruning': 0.36756967426241394, 'lin1_layer2_pruning': 0.23061675754594618, 'lin2_layer2_pruning': 0.15320760593406935, 'lin1_layer3_pruning': 0.16268489705917896, 'lin2_layer3_pruning': 0.28833428947406, 'lin1_layer4_pruning': 0.49636123033528323, 'lin2_layer4_pruning': 0.26194465446744164, 'lin1_layer5_pruning': 0.45114815405117237, 'lin2_layer5_pruning': 0.44509326032812013}. Best is trial 17 with value: 0.977850697292863.\n",
            "<ipython-input-32-4b55b337298f>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "[codecarbon WARNING @ 22:59:34] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 22:59:34] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 22:59:34] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 22:59:34] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 22:59:35] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 22:59:35] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:59:35] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 22:59:35] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 22:59:35] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 22:59:35]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 22:59:35]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 22:59:35]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 22:59:35]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 22:59:35]   CPU count: 2\n",
            "[codecarbon INFO @ 22:59:35]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 22:59:35]   GPU count: 1\n",
            "[codecarbon INFO @ 22:59:35]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 22:59:35] Saving emissions data to file /content/emissions.csv\n",
            "<ipython-input-25-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "[codecarbon INFO @ 22:59:42] Energy consumed for RAM : 0.000812 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:42] Energy consumed for all CPUs : 0.007259 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:42] Energy consumed for all GPUs : 0.008460 kWh. Total GPU Power : 63.872837454267 W\n",
            "[codecarbon INFO @ 22:59:42] 0.016530 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 22:59:48] Energy consumed for RAM : 0.000016 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 22:59:48] Energy consumed for all CPUs : 0.000146 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 22:59:48] Energy consumed for all GPUs : 0.000238 kWh. Total GPU Power : 69.57288913040033 W\n",
            "[codecarbon INFO @ 22:59:48] 0.000400 kWh of electricity used since the beginning.\n",
            "[I 2025-01-29 22:59:48,146] Trial 19 finished with value: 0.9696472518457753 and parameters: {'lin1_layer0_pruning': 0.46715127347788693, 'lin2_layer0_pruning': 0.14957044027790245, 'lin1_layer1_pruning': 0.14251931197973836, 'lin2_layer1_pruning': 0.4412481516459907, 'lin1_layer2_pruning': 0.15552778380314175, 'lin2_layer2_pruning': 0.14490048798005795, 'lin1_layer3_pruning': 0.22489524500730568, 'lin2_layer3_pruning': 0.38934364489109824, 'lin1_layer4_pruning': 0.44945850490328526, 'lin2_layer4_pruning': 0.14595600033540698, 'lin1_layer5_pruning': 0.3712086075024723, 'lin2_layer5_pruning': 0.48819660866236664}. Best is trial 17 with value: 0.977850697292863.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_emissions_vs_accuracy_with_labels(study):\n",
        "    # Extract emissions and accuracy from each trial\n",
        "    emissions = [trial.user_attrs[\"emissions\"] for trial in study.trials if \"emissions\" in trial.user_attrs]\n",
        "    accuracies = [trial.user_attrs[\"accuracy\"] for trial in study.trials if \"accuracy\" in trial.user_attrs]\n",
        "    trial_numbers = [trial.number for trial in study.trials if \"emissions\" in trial.user_attrs]\n",
        "\n",
        "    # Create a scatter plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i, (emission, accuracy) in enumerate(zip(emissions, accuracies)):\n",
        "        plt.scatter(emission, accuracy, color='blue')\n",
        "        plt.text(emission, accuracy, f'{trial_numbers[i]}', color='red', fontsize=9)\n",
        "\n",
        "    plt.title('Model Emissions vs. Accuracy with Trial Labels')\n",
        "    plt.xlabel('Emissions (kWh)')\n",
        "    plt.ylabel('Validation Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Assuming 'study' is your Optuna study object\n",
        "plot_emissions_vs_accuracy_with_labels(study)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "qplquG73SE3n",
        "outputId": "aa4e3da2-e657-4e92-8cbc-fe069f293843"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Collect data from the study into lists\n",
        "trial_numbers = [trial.number for trial in study.trials]\n",
        "accuracies = [trial.value for trial in study.trials]\n",
        "emissions = [trial.user_attrs['emissions'] for trial in study.trials]\n",
        "\n",
        "# Create a DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Trial Number': trial_numbers,\n",
        "    'Validation Accuracy': accuracies,\n",
        "    'Emissions (kWh)': emissions\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oW2NKnAUDYf",
        "outputId": "e0cc1695-dd5b-446b-c4b3-38e55c8bbb8e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Trial Number  Validation Accuracy  Emissions (kWh)\n",
            "0              0             0.969647                0\n",
            "1              1             0.964725                0\n",
            "2              2             0.969647                0\n",
            "3              3             0.952420                0\n",
            "4              4             0.972929                0\n",
            "5              5             0.963084                0\n",
            "6              6             0.976210                0\n",
            "7              7             0.977030                0\n",
            "8              8             0.968007                0\n",
            "9              9             0.970468                0\n",
            "10            10             0.967186                0\n",
            "11            11             0.972929                0\n",
            "12            12             0.972929                0\n",
            "13            13             0.972108                0\n",
            "14            14             0.977030                0\n",
            "15            15             0.971288                0\n",
            "16            16             0.972108                0\n",
            "17            17             0.977851                0\n",
            "18            18             0.976210                0\n",
            "19            19             0.969647                0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df = results_df.sort_values('Emissions (kWh)', ascending=True)\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "print(sorted_df)\n",
        "This sorted DataFram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "YYEB27CXxLyb",
        "outputId": "597a37b0-12cc-4c51-97c1-e93df3d7c4de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-c4556b490328>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c4556b490328>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    This sorted DataFram\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}
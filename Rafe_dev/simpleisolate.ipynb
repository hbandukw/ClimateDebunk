{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10596106,"sourceType":"datasetVersion","datasetId":6558503}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install nlpaug\n!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:43:14.980808Z","iopub.execute_input":"2025-01-28T16:43:14.981147Z","iopub.status.idle":"2025-01-28T16:43:23.061873Z","shell.execute_reply.started":"2025-01-28T16:43:14.981107Z","shell.execute_reply":"2025-01-28T16:43:23.060893Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.12.14)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\nRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\nfrom sklearn.metrics import f1_score, confusion_matrix, balanced_accuracy_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.optim import AdamW, lr_scheduler\n# import nlpaug.augmenter.word as naw\nimport optuna\nimport shutil\nimport zipfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:43:23.062839Z","iopub.execute_input":"2025-01-28T16:43:23.063133Z","iopub.status.idle":"2025-01-28T16:44:08.978051Z","shell.execute_reply.started":"2025-01-28T16:43:23.063103Z","shell.execute_reply":"2025-01-28T16:44:08.977360Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"To compare, this is the result of the original run: \n- Trial 0 finished with value: 0.6890894175553732 and parameters: {'learning_rate': 1.4818151091980784e-05, 'num_trainable_layers': 4, 'dropout_rate': 0.119110690825033, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8960630325737292, 'epochs': 3}. Best is trial 0 with value: 0.6890894175553732. 3083.8s 66 [I 2025-01-24 09:37:48,516]\n- Trial 1 finished with value: 0.8728465955701394 and parameters: {'learning_rate': 2.517549631081625e-05, 'num_trainable_layers': 6, 'dropout_rate': 0.41403406363825646, 'batch_size': 16, 'step_size': 4, 'gamma': 0.42358721860354653, 'epochs': 5}. Best is trial 1 with value: 0.8728465955701394. 4771.2s 67 [I 2025-01-24 10:05:55,931]\n- Trial 2 finished with value: 0.8884331419196062 and parameters: {'learning_rate': 5.202379803067906e-05, 'num_trainable_layers': 3, 'dropout_rate': 0.298865675518514, 'batch_size': 32, 'step_size': 5, 'gamma': 0.7444522192227857, 'epochs': 5}. Best is trial 2 with value: 0.8884331419196062. 5828.5s 68 [I 2025-01-24 10:23:33,239]\n- Trial 3 finished with value: 0.7875307629204266 and parameters: {'learning_rate': 0.00016564874914610164, 'num_trainable_layers': 4, 'dropout_rate': 0.24484638353248456, 'batch_size': 32, 'step_size': 8, 'gamma': 0.7133676325508109, 'epochs': 3}. Best is trial 2 with value: 0.8884331419196062. 7368.5s 69 [I 2025-01-24 10:49:13,274]\n- Trial 4 finished with value: 0.8310090237899918 and parameters: {'learning_rate': 6.24880645646062e-05, 'num_trainable_layers': 6, 'dropout_rate': 0.1787526406980196, 'batch_size': 16, 'step_size': 9, 'gamma': 0.4218077557604636, 'epochs': 4}. Best is trial 2 with value: 0.8884331419196062. 9207.9s 70 [I 2025-01-24 11:19:52,632] ","metadata":{}},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/balancedfull/df1.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/balancedfull/df2.csv\")\ndf3 = pd.read_csv(\"/kaggle/input/balancedfull/df3.csv\")\ndf4 = pd.read_csv(\"/kaggle/input/balancedfull/df4.csv\")\ndf5 = pd.read_csv(\"/kaggle/input/balancedfull/df5.csv\")\n\ndf_balanced1 = pd.read_csv(\"/kaggle/input/balancedfull/df_balanced1.csv\")\ndf_balanced2 = pd.read_csv(\"/kaggle/input/balancedfull/df_balanced2.csv\")\ndf_balanced3 = pd.read_csv(\"/kaggle/input/balancedfull/df_balanced3.csv\")\ndf_balanced4 = pd.read_csv(\"/kaggle/input/balancedfull/df_balanced4.csv\")\ndf_balanced5 = pd.read_csv(\"/kaggle/input/balancedfull/df_balanced5.csv\")\n\ndf1_text = df1['quote']\ndf2_text = df2['quote']\ndf3_text = df3['quote']\ndf4_text = df4['quote']\ndf5_text = df5['quote']\n\ndf1_label = df1['numeric_label']\ndf2_label = df2['numeric_label']\ndf3_label = df3['numeric_label']\ndf4_label = df4['numeric_label']\ndf5_label = df5['numeric_label']\n\ndf_balanced1_text = df_balanced1['quote']\ndf_balanced2_text = df_balanced2['quote']\ndf_balanced3_text = df_balanced3['quote']\ndf_balanced4_text = df_balanced4['quote']\ndf_balanced5_text = df_balanced5['quote']\n\ndf_balanced1_label = df_balanced1['numeric_label']\ndf_balanced2_label = df_balanced2['numeric_label']\ndf_balanced3_label = df_balanced3['numeric_label']\ndf_balanced4_label = df_balanced4['numeric_label']\ndf_balanced5_label = df_balanced5['numeric_label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:44:08.978916Z","iopub.execute_input":"2025-01-28T16:44:08.979459Z","iopub.status.idle":"2025-01-28T16:44:09.280966Z","shell.execute_reply.started":"2025-01-28T16:44:08.979433Z","shell.execute_reply":"2025-01-28T16:44:09.280294Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"datasets = [df_balanced1, df_balanced2, df_balanced3, df_balanced4]\n\n# Extract quotes and labels using list comprehension\ntexts = [ds['quote'] for ds in datasets]\nlabels = [ds['numeric_label'] for ds in datasets]\n\n# Concatenate all texts and labels using pandas.concat\ntrain_texts = pd.concat(texts, ignore_index=True)\ntrain_labels = pd.concat(labels, ignore_index=True)\n\nval_texts = df5_text = df5['quote']\nval_labels = df5_label = df5['numeric_label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:44:09.281689Z","iopub.execute_input":"2025-01-28T16:44:09.281890Z","iopub.status.idle":"2025-01-28T16:44:09.286990Z","shell.execute_reply.started":"2025-01-28T16:44:09.281873Z","shell.execute_reply":"2025-01-28T16:44:09.286144Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:44:09.287746Z","iopub.execute_input":"2025-01-28T16:44:09.288065Z","iopub.status.idle":"2025-01-28T16:44:09.367295Z","shell.execute_reply.started":"2025-01-28T16:44:09.288036Z","shell.execute_reply":"2025-01-28T16:44:09.366331Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\nMAX_LENGTH = 365\n\n# Dataset and DataLoader preparation\nclass QuotesDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ndef encode_data(tokenizer, texts, labels, max_length):\n    try:\n        if isinstance(texts, pd.Series):\n            texts = texts.tolist()\n        if isinstance(labels, pd.Series):\n            labels = labels.tolist()\n            \n        encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        return QuotesDataset(encodings, labels)\n\n    except Exception as e:\n        print(f\"Error during tokenization: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:44:09.369271Z","iopub.execute_input":"2025-01-28T16:44:09.369510Z","iopub.status.idle":"2025-01-28T16:44:10.174893Z","shell.execute_reply.started":"2025-01-28T16:44:09.369490Z","shell.execute_reply":"2025-01-28T16:44:10.174216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51a211951f064a47ab0bb96ff47331c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"696c757a53954c13b30e3904c0badc37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75eedada9c034370b999d3c3c5db66ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d847a51e2094430a01ba9284bf8be9b"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_dataset = encode_data(tokenizer, train_texts, train_labels, MAX_LENGTH)\nval_dataset = encode_data(tokenizer, val_texts, val_labels, MAX_LENGTH) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:44:10.175930Z","iopub.execute_input":"2025-01-28T16:44:10.176158Z","iopub.status.idle":"2025-01-28T16:44:24.889310Z","shell.execute_reply.started":"2025-01-28T16:44:10.176137Z","shell.execute_reply":"2025-01-28T16:44:24.888366Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**Objective1: Run a few trials of original model + early stopping + higher epochs max (increase from 5 to 10)**","metadata":{}},{"cell_type":"markdown","source":"Optimization: ","metadata":{}},{"cell_type":"code","source":"def modify_model(model, num_trainable_layers, dropout_rate):\n    # Freeze layers: only the last 'num_trainable_layers' are trainable\n    total_layers = len(model.distilbert.transformer.layer)\n    for layer_index, layer in enumerate(model.distilbert.transformer.layer):\n        if layer_index < total_layers - num_trainable_layers:\n            for param in layer.parameters():\n                param.requires_grad = False\n\n    # Adjust dropout rates in applicable transformer layers\n    for layer in model.distilbert.transformer.layer:\n        layer.attention.dropout.p = dropout_rate\n        layer.ffn.dropout.p = dropout_rate\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:44:24.890221Z","iopub.execute_input":"2025-01-28T16:44:24.890436Z","iopub.status.idle":"2025-01-28T16:44:24.895066Z","shell.execute_reply.started":"2025-01-28T16:44:24.890418Z","shell.execute_reply":"2025-01-28T16:44:24.894280Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, device):\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n    average_loss = train_loss / len(train_loader)\n    accuracy = correct_train / total_train\n    return average_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:44:24.895823Z","iopub.execute_input":"2025-01-28T16:44:24.896012Z","iopub.status.idle":"2025-01-28T16:44:24.909466Z","shell.execute_reply.started":"2025-01-28T16:44:24.895995Z","shell.execute_reply":"2025-01-28T16:44:24.908860Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def validate_model(model, val_loader, device):\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    all_predictions = []\n    all_true_labels = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_true_labels.extend(batch['labels'].cpu().numpy())\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n    average_val_loss = val_loss / len(val_loader)\n    accuracy = correct_val / total_val\n    return average_val_loss, accuracy, all_predictions, all_true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:44:24.910377Z","iopub.execute_input":"2025-01-28T16:44:24.910640Z","iopub.status.idle":"2025-01-28T16:44:24.926243Z","shell.execute_reply.started":"2025-01-28T16:44:24.910620Z","shell.execute_reply":"2025-01-28T16:44:24.925486Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def objective(trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n    num_trainable_layers = trial.suggest_int('num_trainable_layers', 1, 6)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n    step_size = trial.suggest_int('step_size', 1, 10)\n    gamma = trial.suggest_float('gamma', 0.1, 0.9)\n    epochs = trial.suggest_int('epochs', 2, 10) \n\n    # Early stopping criteria\n    patience = 3\n    min_delta = 0.001\n    best_val_accuracy = 0\n    no_improve_epochs = 0\n    \n    # Model setup and modification\n    model_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n    model = DistilBertForSequenceClassification(model_config)\n    model = modify_model(model, num_trainable_layers, dropout_rate)\n    model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    # Training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    val_accuracies = []\n\n    for epoch in range(epochs):\n        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, device)\n        val_loss, val_accuracy, _, _ = validate_model(model, val_loader, device)\n        scheduler.step()\n\n        if val_accuracy > best_val_accuracy + min_delta:\n            best_val_accuracy = val_accuracy\n            no_improve_epochs = 0\n        else:\n            no_improve_epochs += 1\n\n        if no_improve_epochs >= patience:\n            print(f\"Stopping early at epoch {epoch+1}\")\n            break\n    \n    # for epoch in range(epochs):\n    #     train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, device)\n    #     val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n    #     scheduler.step()\n\n    #     # Collect metrics\n    #     val_accuracies.append(val_accuracy)\n\n    # file_path = f\"/kaggle/working/output_{trial.number}.pth\"\n    # torch.save(model.state_dict(), file_path)\n\n    # Store the best or last validation accuracy\n    # best_val_accuracy = max(val_accuracies)  # or you could use val_accuracies[-1] for the last\n\n    return best_val_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:45:20.202689Z","iopub.execute_input":"2025-01-28T16:45:20.203048Z","iopub.status.idle":"2025-01-28T16:45:20.210730Z","shell.execute_reply.started":"2025-01-28T16:45:20.203014Z","shell.execute_reply":"2025-01-28T16:45:20.209696Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=5)  # Adjust the number of trials as needed\n\nprint(\"Best trial:\")\nprint(study.best_trial.params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T16:45:24.142624Z","iopub.execute_input":"2025-01-28T16:45:24.142915Z","iopub.status.idle":"2025-01-28T19:03:49.412751Z","shell.execute_reply.started":"2025-01-28T16:45:24.142893Z","shell.execute_reply":"2025-01-28T19:03:49.411594Z"}},"outputs":[{"name":"stderr","text":"[I 2025-01-28 16:45:24,144] A new study created in memory with name: no-name-83b9c072-a99a-4d9c-8b83-5a61d6bb641e\n<ipython-input-6-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n[I 2025-01-28 17:30:02,491] Trial 0 finished with value: 0.2947454844006568 and parameters: {'learning_rate': 0.0003786550610492913, 'num_trainable_layers': 4, 'dropout_rate': 0.32010594371594264, 'batch_size': 16, 'step_size': 8, 'gamma': 0.33452792460926456, 'epochs': 10}. Best is trial 0 with value: 0.2947454844006568.\n","output_type":"stream"},{"name":"stdout","text":"Stopping early at epoch 8\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-01-28 17:40:43,969] Trial 1 finished with value: 0.12643678160919541 and parameters: {'learning_rate': 0.000638182315640936, 'num_trainable_layers': 3, 'dropout_rate': 0.1336002179576891, 'batch_size': 64, 'step_size': 5, 'gamma': 0.6350661291473785, 'epochs': 2}. Best is trial 0 with value: 0.2947454844006568.\n[I 2025-01-28 18:10:15,157] Trial 2 finished with value: 0.5566502463054187 and parameters: {'learning_rate': 2.05713683503022e-05, 'num_trainable_layers': 5, 'dropout_rate': 0.41690115957387186, 'batch_size': 16, 'step_size': 9, 'gamma': 0.40762971238383405, 'epochs': 5}. Best is trial 2 with value: 0.5566502463054187.\n[I 2025-01-28 18:39:30,692] Trial 3 finished with value: 0.5665024630541872 and parameters: {'learning_rate': 0.00015533133816326115, 'num_trainable_layers': 1, 'dropout_rate': 0.143048848905019, 'batch_size': 16, 'step_size': 7, 'gamma': 0.5469729079801496, 'epochs': 8}. Best is trial 3 with value: 0.5665024630541872.\n","output_type":"stream"},{"name":"stdout","text":"Stopping early at epoch 6\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-01-28 19:03:49,407] Trial 4 finished with value: 0.5796387520525451 and parameters: {'learning_rate': 0.00011601732158263483, 'num_trainable_layers': 1, 'dropout_rate': 0.25521230309869525, 'batch_size': 32, 'step_size': 9, 'gamma': 0.10224710801514512, 'epochs': 7}. Best is trial 4 with value: 0.5796387520525451.\n","output_type":"stream"},{"name":"stdout","text":"Stopping early at epoch 5\nBest trial:\n{'learning_rate': 0.00011601732158263483, 'num_trainable_layers': 1, 'dropout_rate': 0.25521230309869525, 'batch_size': 32, 'step_size': 9, 'gamma': 0.10224710801514512, 'epochs': 7}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"**Objective2: Run a few trials of original model + gradient clipping**","metadata":{}},{"cell_type":"code","source":"def train_one_epoch_clipping(model, train_loader, optimizer, grad_clip, device):\n    model.train()\n    train_loss = 0\n    correct_train = 0\n    total_train = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        train_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n    average_loss = train_loss / len(train_loader)\n    accuracy = correct_train / total_train\n    return average_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:03:55.589958Z","iopub.execute_input":"2025-01-28T19:03:55.590382Z","iopub.status.idle":"2025-01-28T19:03:55.598393Z","shell.execute_reply.started":"2025-01-28T19:03:55.590342Z","shell.execute_reply":"2025-01-28T19:03:55.597484Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def objective(trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n    num_trainable_layers = trial.suggest_int('num_trainable_layers', 1, 6)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n    step_size = trial.suggest_int('step_size', 1, 10)\n    gamma = trial.suggest_float('gamma', 0.1, 0.9)\n    epochs = trial.suggest_int('epochs', 2, 5) \n    grad_clip = 1.0\n\n    # Model setup and modification\n    model_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=8)\n    model = DistilBertForSequenceClassification(model_config)\n    model = modify_model(model, num_trainable_layers, dropout_rate)\n    model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    # Training and validation\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    val_accuracies = []\n\n    for epoch in range(epochs):\n        train_loss, train_accuracy = train_one_epoch_clipping(model, train_loader, optimizer, grad_clip, device)\n        val_loss, val_accuracy, all_predictions, all_true_labels = validate_model(model, val_loader, device)\n        scheduler.step()\n\n        # Collect metrics\n        val_accuracies.append(val_accuracy)\n\n    best_val_accuracy = max(val_accuracies)\n\n    return best_val_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:04:24.102252Z","iopub.execute_input":"2025-01-28T19:04:24.102594Z","iopub.status.idle":"2025-01-28T19:04:24.109761Z","shell.execute_reply.started":"2025-01-28T19:04:24.102563Z","shell.execute_reply":"2025-01-28T19:04:24.108787Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=5)  # Adjust the number of trials as needed\n\nprint(\"Best trial:\")\nprint(study.best_trial.params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T19:04:27.403771Z","iopub.execute_input":"2025-01-28T19:04:27.404090Z","iopub.status.idle":"2025-01-28T20:46:54.284498Z","shell.execute_reply.started":"2025-01-28T19:04:27.404064Z","shell.execute_reply":"2025-01-28T20:46:54.283594Z"}},"outputs":[{"name":"stderr","text":"[I 2025-01-28 19:04:27,405] A new study created in memory with name: no-name-4fce4f5c-7b3b-4912-898c-9e423bba40c2\n<ipython-input-6-2d9dfced9ac1>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n[I 2025-01-28 19:19:55,118] Trial 0 finished with value: 0.5279146141215106 and parameters: {'learning_rate': 0.00023971978599227062, 'num_trainable_layers': 2, 'dropout_rate': 0.4892850221795769, 'batch_size': 16, 'step_size': 2, 'gamma': 0.11922139670221972, 'epochs': 3}. Best is trial 0 with value: 0.5279146141215106.\n[I 2025-01-28 19:43:10,357] Trial 1 finished with value: 0.5188834154351396 and parameters: {'learning_rate': 0.00019622431960104585, 'num_trainable_layers': 5, 'dropout_rate': 0.13237435636268813, 'batch_size': 64, 'step_size': 3, 'gamma': 0.8425947552179673, 'epochs': 4}. Best is trial 0 with value: 0.5279146141215106.\n[I 2025-01-28 19:58:30,561] Trial 2 finished with value: 0.26929392446633826 and parameters: {'learning_rate': 0.0004739541634733061, 'num_trainable_layers': 2, 'dropout_rate': 0.10602165337807024, 'batch_size': 32, 'step_size': 1, 'gamma': 0.7315819294393705, 'epochs': 3}. Best is trial 0 with value: 0.5279146141215106.\n[I 2025-01-28 20:16:59,728] Trial 3 finished with value: 0.14532019704433496 and parameters: {'learning_rate': 0.00018887309983662515, 'num_trainable_layers': 6, 'dropout_rate': 0.2177622813509403, 'batch_size': 16, 'step_size': 4, 'gamma': 0.3782459930082863, 'epochs': 3}. Best is trial 0 with value: 0.5279146141215106.\n[I 2025-01-28 20:46:54,279] Trial 4 finished with value: 0.2947454844006568 and parameters: {'learning_rate': 0.000990389138331645, 'num_trainable_layers': 6, 'dropout_rate': 0.4478527660450413, 'batch_size': 32, 'step_size': 10, 'gamma': 0.8497685006265375, 'epochs': 5}. Best is trial 0 with value: 0.5279146141215106.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\n{'learning_rate': 0.00023971978599227062, 'num_trainable_layers': 2, 'dropout_rate': 0.4892850221795769, 'batch_size': 16, 'step_size': 2, 'gamma': 0.11922139670221972, 'epochs': 3}\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"**Objective 3: Run a few trials of original model + switching training / testing**","metadata":{}},{"cell_type":"markdown","source":"This aproach is aiming to increase the hyperparameter's generalizability. However, due to the limit time left, this will be left to the end of the project as a stretch goal. ","metadata":{}}]}
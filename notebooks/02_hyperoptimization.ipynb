{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script used for getting optimal hyperparameters\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\n",
    "from sklearn.metrics import f1_score, confusion_matrix, balanced_accuracy_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import shutil\n",
    "import zipfile\n",
    "import yaml\n",
    "\n",
    "from src.data_prep import read_train_data, read_val_data\n",
    "\n",
    "# Load configuration\n",
    "def load_config(config_path=\"configs/hyperoptim_config.yaml\"):\n",
    "    \"\"\"Loads YAML configuration file.\"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def setup_model_for_hyperopt(num_trainable_layers, dropout_rate):\n",
    "    config = DistilBertConfig.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=8,\n",
    "        dropout=dropout_rate,\n",
    "        attention_dropout=dropout_rate\n",
    "    )\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for layer_idx in range(6 - num_trainable_layers, 6):\n",
    "        for name, param in model.distilbert.transformer.layer[layer_idx].named_parameters():\n",
    "            param.requires_grad = True\n",
    "    for name, param in model.classifier.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "import pandas as pd\n",
    "from ..src.model import load_model\n",
    "from ..src.data_prep import encode_data, QuotesDataset\n",
    "from ..src.utils import calculate_f1_score, plot_confusion_matrix, plot_precision_recall, plot_roc_curve\n",
    "from ..src import config\n",
    "\n",
    "def load_test_data(testpath: str):\n",
    "    \"\"\"Loads test data from the specified path.\"\"\"\n",
    "    if testpath.endswith(\".csv\"):\n",
    "        test = pd.read_csv(testpath)\n",
    "    elif testpath.endswith(\".parquet\"):\n",
    "        test = pd.read_parquet(testpath)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only CSV and Parquet are supported.\")\n",
    "    return test['quote'], test['numeric_label']\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    all_test_labels = []\n",
    "    all_test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            test_loss += outputs.loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct_test += (predictions == batch['labels']).sum().item()\n",
    "            total_test += batch['labels'].size(0)\n",
    "            all_test_labels.extend(batch['labels'].cpu().numpy())\n",
    "            all_test_preds.extend(predictions.cpu().numpy())\n",
    "    average_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = correct_test / total_test\n",
    "    f1 = calculate_f1_score(all_test_labels, all_test_preds)\n",
    "    return average_test_loss, accuracy, f1, all_test_labels, all_test_preds\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = load_model() \n",
    "    model.load_state_dict(torch.load(config[\"trained_model_path\"], map_location=torch.device(\"cpu\")))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    test_texts, test_labels = load_test_data(config[\"testpath\"])\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(config[\"tokenizer_model\"], do_lower_case=True)\n",
    "    test_dataset = encode_data(tokenizer, test_texts, test_labels, config[\"max_length\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    test_loss, test_accuracy, test_f1, test_labels, test_preds = test_model(model, test_loader, device)\n",
    "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Test F1: {test_f1}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(test_labels, test_preds, classes=[str(i) for i in range(8)])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

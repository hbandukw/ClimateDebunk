{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.model_selection import KFold\n",
    "from config import load_config\n",
    "from data_prep import read_data\n",
    "from augment import balance_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "augmentation_config = load_config(\"configs/augmentation_config.yaml\")\n",
    "\n",
    "# Set device\n",
    "device = (\n",
    "    augmentation_config[\"device\"]\n",
    "    if augmentation_config[\"device\"] in [\"cuda\", \"cpu\"]\n",
    "    else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "# Read data to be augmented\n",
    "df = read_data(augmentation_config['data_path'])\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(augmentation_config[\"output_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation model\n",
    "augmenter = naw.ContextualWordEmbsAug(\n",
    "    model_path=augmentation_config[\"augmenter_model\"],\n",
    "    action=augmentation_config[\"augment_action\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# K-Fold splitting & augmentation\n",
    "kf = KFold(\n",
    "    n_splits=augmentation_config[\"n_splits\"],\n",
    "    shuffle=True,\n",
    "    random_state=augmentation_config[\"random_seed\"],\n",
    ")\n",
    "\n",
    "balanced_datasets = []\n",
    "for i, (_, test_index) in enumerate(kf.split(df), start=1):\n",
    "    subset = df.iloc[test_index]\n",
    "    balanced_subset = balance_dataset(subset, \"numeric_label\", \"quote\", augmenter)\n",
    "    if augmentation_config['save_intermediate_file'] == True:\n",
    "        balanced_path = os.path.join(\n",
    "        augmentation_config[\"output_dir\"], f\"df_balanced{i}.csv\"\n",
    "    )\n",
    "        balanced_subset.to_csv(balanced_path, index=False)\n",
    "    else:\n",
    "        pass\n",
    "    print(f\"Augmented subset {i} saved to {balanced_path}\")\n",
    "    balanced_datasets.append(balanced_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the balanced subsets\n",
    "combined_balanced = pd.concat(balanced_datasets, ignore_index=True)\n",
    "\n",
    "# Save combined balanced dataset\n",
    "output_path = os.path.join(augmentation_config[\"output_dir\"], \"aug_balanced_train.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AutoConfig\n",
    "from ..src.model import load_model\n",
    "from ..src.data_prep import create_data_loader\n",
    "from ..src.utils import plot_loss, plot_accuracy, calculate_f1_score\n",
    "\n",
    "\n",
    "# Function to load config\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def convert_to_onnx(onnx_path, model):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch model to ONNX format.\n",
    "\n",
    "    Args:\n",
    "        onnx_path (str): Path to save the ONNX model.\n",
    "    \"\"\"\n",
    "    dummy_input = {\n",
    "        \"input_ids\": torch.randint(0, 30522, (1, 365)),  # Random token IDs\n",
    "        \"attention_mask\": torch.ones((1, 365))  # Full attention mask\n",
    "    }\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "        onnx_path,\n",
    "        input_names=[\"input_ids\", \"attention_mask\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\"input_ids\": {0: \"batch_size\"}, \"attention_mask\": {0: \"batch_size\"}},\n",
    "        opset_version=14  # ONNX opset for transformers\n",
    "    )\n",
    "    return \n",
    "\n",
    "def validate_onnx_model(session, val_loader):\n",
    "    print(\"Starting ONNX evaluation...\")\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        print(f\"Processing batch {batch_idx+1}...\")\n",
    "        # Ensure inputs are numpy arrays\n",
    "        input_ids = np.array(batch[\"input_ids\"], dtype=np.int64)\n",
    "        attention_mask = np.array(batch[\"attention_mask\"], dtype=np.int64)\n",
    "        labels = np.array(batch[\"labels\"], dtype=np.int64)\n",
    "\n",
    "        # Prepare ONNX input dictionary\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = session.run([\"logits\"], inputs)\n",
    "        predictions = np.argmax(outputs[0], axis=1)\n",
    "\n",
    "        # Compute accuracy\n",
    "        batch_correct = np.sum(predictions == labels)\n",
    "        total_correct += batch_correct\n",
    "        total_samples += len(labels)\n",
    "        all_val_labels.extend(labels)\n",
    "        all_val_preds.extend(predictions)\n",
    "        print(f\"Batch {batch_idx+1}: {batch_correct}/{len(labels)} correct\")\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    f1 = calculate_f1_score(all_val_labels, all_val_preds)\n",
    "    print(f\"ONNX Validation Accuracy: {accuracy:.4f} ({total_correct}/{total_samples})\")\n",
    "    print(f\"ONNX Validation F1 Score: {f1:.4f}\")\n",
    "    return accuracy, f1\n",
    "\n",
    "\n",
    "def apply_dynamic_quant(onnx_path, quantized_onnx_path):\n",
    "    \"\"\"\n",
    "    Apply dynamic quantization to the ONNX model.\n",
    "\n",
    "    Args:\n",
    "        onnx_path (str): Path to the input ONNX model.\n",
    "        quantized_onnx_path (str): Path to save the quantized ONNX model.\n",
    "    \"\"\"\n",
    "    quantize_dynamic(\n",
    "        model_input=onnx_path,\n",
    "        model_output=quantized_onnx_path,\n",
    "        weight_type=QuantType.QInt8  # Quantizes weights to int8\n",
    "    )\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load Configs\n",
    "    main_config = load_config('configs/config.yaml')\n",
    "    quant_config = load_config('configs/quantization_config.yaml')\n",
    "\n",
    "    # Load Fine-tuned Model\n",
    "    model = load_model()\n",
    "    model.load_state_dict(torch.load(main_config[\"trained_model_path\"], map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "\n",
    "    # Convert to ONNX\n",
    "    convert_to_onnx(model, quant_config['onnx_path'])\n",
    "\n",
    "    # Dynamic Quantization\n",
    "    apply_dynamic_quant(quant_config['onnx_path'], quant_config['quantized_onnx_path'])\n",
    "\n",
    "    # Load Validation Data\n",
    "    val_loader = create_data_loader(\n",
    "        filepath=main_config[\"validation_data_path\"],\n",
    "        label_column=main_config[\"label_column\"],\n",
    "        tokenizer_model=main_config[\"tokenizer_model\"],\n",
    "        max_length=main_config[\"max_length\"],\n",
    "        batch_size=main_config[\"batch_size\"],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Load ONNX Model\n",
    "    session = onnxruntime.InferenceSession(quant_config['quantized_onnx_path'])\n",
    "\n",
    "    # Validate ONNX Model\n",
    "    validate_onnx_model(session, val_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
